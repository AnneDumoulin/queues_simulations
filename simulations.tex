
\documentclass{scrartcl}
%\documentclass[a5paper]{article}

% als je de antwoorden aan het eind wil hebben, gebruik dan deze regel

\usepackage[solutionfiles]{optional}

% Als je daarentegen de antwoorden onder de opdracht wil hebben, quote
% dan bovenstaande uit, en gebruik de regel hieronder
%\usepackage[nosolutionfiles]{optional}

\opt{nosolutionfiles}{\usepackage[nosolutionfiles]{answers}}
\opt{solutionfiles}{\usepackage{answers}}

\usepackage{preamble}

\newcommand{\notet}[1]{\textcolor{red}{#1}}

% handle indentation below figure captions
\setcapwidth[c]{.8\textwidth}
\setcapwidth{0.95\textwidth}
%\setcapindent{0pt}
%\addtokomafont{caption}{\centering}

%\usepackage[top=5mm, left=5mm, right=5mm, bottom=2cm]{geometry}

\newenvironment{pyconcodeblock}%
 {\VerbatimEnvironment
  \begin{VerbatimOut}{temp.py}}%
 {\end{VerbatimOut}%
  \pyconc{exec(compile(open('temp.py', 'rb').read(), 'temp.py', 'exec'))}%
  \inputpygments{python}{temp.py}}





\title{Queueing simulations}
\author{N.D. Van Foreest and E.R. Van Beesten}
\begin{document}
\maketitle
\tableofcontents

\begin{itemize}
\item document in de levelcrossing illustration directory. 
\item Kijk even goed wat ik eigenlijk allemaal al heb.
\item Overshoot verdeling? 
\item Control zoals bij de psychiaters. Maak de empirische verdelingsfunctie van de wachttijd voor deze queue. Dit lijkt ook op een supermarkt. 
\item Simulation of a D policy of wachttijd berekening ipv queue lengte.
\item Heat bath
\item Nurse routing or assembly line
\item shift sheduling. setups. 
\end{itemize}

\Opensolutionfile{hint}
\Opensolutionfile{ans}

\section{Introduction}

Typically a queueing system is subject to rules about when to allow jobs to enter the system or to adapt the service capacity. Such a decision rule is called a \emph{policy}.  The theoretical analysis of the efficacy of policies is often very hard, while with simulation it becomes doable.  In this document we present a number of cases to see how simulation can be used to analyze and improve queueing systems. Besides the fact that these the cases will improve your understanding of queueing systems and probability theory, they will also make clear  that simulation is a really creative activity and involves solving many interesting and challenging  algorithmic problems.  


Each case is organized in a number of exercises. For each exercise,
\begin{enumerate}
\item Make a design of how you want to solve the problem. For instance, make a model of a queueing system, or a control policy structure, or compute relevant KPIs (key performance indicators, such as cost, or utilization of the server, and so on). In other words, think before you type. 
\item Try to translate your ideas into pseudo code or, better yet,  python\footnote{Some of you might wonder why we use python rather than R. There are a few reasons for this. Python is more or less the third most used programming language, after C++ and java. It is widely used by companies, while R is hardly  used outside academia. Programming OR applications is easier in python; it will also used in other courses. Finally, if you are interested in machine learning and artificial intelligence, python i, hands-down the best choice.}
  \item If you don't succeed in getting your program to work,  look up the code written by us and type it into your python environment.\footnote{Typing yourself forces you to read the code well.}.
  \item Simulate a number of scenarios by varying parameter settings and see what happens.
\end{enumerate}

We expect you to work in a groups of 2 to 3 students and bring a laptop with an \emph{installed and working} python environment, preferably  the anaconda package available at \url{https://www.anaconda.com/},  as this contains all functionality we will need\footnote{There are also python environments available on the web, such as repl.it., but that is typically a bit less practical than running the code on your own machine.}. You can find a nice tutorial to python programming at  \url{https://www.programiz.com/python-programming}. Note, this site advises to install python just by itself. We instead advise you to download anaconda, as this contains also the required numerical libraries. 

We will use the following libraries of python a lot:
\begin{itemize}
\item \pyv{numpy}  provides an enormous amount of functions to handle large (multi-dimensional) arrays with numbers. 
\item \pyv{scipy} contains numerical recipes, such as solvers for optimization software, solvers for differential equations. \pyv{scipy.stats} contains many probability distributions and numerical methods to operate on these functions. 
\item \pyv{matplotlib} provides plotting functionality.
\end{itemize}

Our code is not the most efficient, or fast. Rather, we focus on clarity of code so that the underlying reasoning is as clear as possible. Once our ideas and code are correct, we can start optimizing, if this is necessary. 

Finally, the code is part of the course, hence of the midterms and the exams.  Unless indicated as not obligatory, you have to be able to read the code and understand it.


\clearpage
\section{Exponential distribution}

The aim of this tutorial is to show, empirically, a fascinating fact: even for very small populations in which individuals decide independently to visit a server (a shop, a hospital, etc),  the  exponential distribution is a good model for the interarrival times as seen by the server.  We will develop a simulation to motivate this `fact of nature'.  In particular, our aim is to build analogues to Figure 1.1, 1.2., and 1.3 of the queueing book\footnote{You can find this here:
\url{https://github.com/ndvanforeest/queueing_book}}, but in terms of cdfs instead of pdfs. (Read the description that underlies these figures.)


\begin{exercise}
  Make a plan of the steps you have to carry out to make Figure 1.1 of the queueing book. In the next set of exercises we'll carry out these steps. So please do not read on before having thought about this problem, but   spend some 5 minutes to think about how to approach the problem and how to chop it up into simple steps. Then organize the steps into a logical sequence. Don't worry at first about how to convert your ideas into computer code. Coding is a separate activity.  (As a matter of fact, I always start with making a small plan on how to turn an idea into code, and I call this step `modeling'. Typicallly this is a creative step, and not easy.) 

  \begin{solution}
    \begin{enumerate}
    \item Generate realizations of a uniformly distributed random variable representing the interarrival times of one customer.
    \item Plot the interarrival times.
    \item Compute the (empirical) distribution function of the simulated interarrival times.
    \item Plot the (empirical) distribution function.
    \item Generate realizations of uniformly distributed random variables representing the interarrival times of multiple customers, e.g., 3. 
    \item Compute the arrival times for each customer.
    \item Merge  the arrival times for all customers. This is the arrival process as seen by the shop.
    \item Compute the interarrival times as seen by the shop.
    \item Plot these interarrival times.
    \item Compare to the exponential distribution function with a suitable arrival rate $\lambda$. 
    \end{enumerate}
  \end{solution}
\end{exercise}

We need some python libraries to make our lives a bit easier. You should copy this code into your editor.

\begin{pyblock}
import numpy as np
import scipy
import matplotlib
matplotlib.use('pdf') 
import matplotlib.pyplot as plt

# this is to print not too many digits to the screen
np.set_printoptions(precision=3) 
\end{pyblock}


\subsection{Empirical distributions}
\label{sec:empir-distr}

One important step in this process is to compute the empirical distribution. As this is much more interesting (and challenging) than you might think\footnote{If you search the web, you will see that computing the empirical density function is even more challenging.}, we start with this. Once we can compute empirical distribution functions, we are in good shape to set up the rest of the simulation. 

Before designing an algorithm to compute , it is best to start with a simple numerical example and try to formalize the steps we take in the process.

\begin{exercise}
  Suppose you are given the following numbers: \pyb{a = [3.2, 4, 4, 1.3, 8.5, 9]}. What steps do you take to make the empirical distribution function? Recall, this is defined as
  \begin{equation}
    \label{eq:1}
    F(x) = \frac{\# \{i : a_i \leq x\}}{n}, 
  \end{equation}
  with $n$ is the length of $a$.

Can you turn it into an algorithm? (Just attempt to design an algorithm. Even if you don't succeed, trying is important. Then read the code in the solution.)

  \begin{solution}
We put the algorithm in a function so that we can use it later.  The algorithm is useful to study,  but it has some weak points. In the exercses below we will repair the problems. 
    \begin{pyblock}
def cdf(a):
    a = sorted(a)
    m, M = int(min(a)), int(max(a))+1
    # Since we know that a is sorted, this next line 
    # would be better, but less clear perhaps: 
    # m, M = int(a[0]), int(a[-1])+1 

    F = dict() # store the function i \to F[i]
    F[m-1]=0  # since F[x] = 0 for all x < m
    i = 0
    for x in range(m, M):
        F[x] = F[x-1]
        while i< len(a) and a[i] <= x:
            F[x] += 1
            i += 1

    # normalize
    for x in F.keys(): 
        F[x] /= len(a)

    return F
    \end{pyblock}

Now run  this to see the result.
\begin{pyblock}
F = cdf(a)
print(F)
\end{pyblock}


  \end{solution}
\end{exercise}

\begin{exercise}\label{ex:2}
  The method provided by the (solution of the) previous exercise is simple, but not completely correct. What is wrong?
  \begin{solution}
    We have to guess the support of $F$ (the set of points where $F$ makes the jumps) upfront, and we concentrated the support on the integers. However $F$ makes jumps at floats, for instance  at $3.2$. 
  \end{solution}
\end{exercise}

A better idea is to consider the sorted version $s$ of $a$. 

\begin{exercise}
  Sort the numbers in the list $a$; let this be $s$.  Make a plot (by hand) of $s$.  Now observe the important fact that $i\to s_i$ is the inverse of the distribution $F$ (except for the normalization).
  \begin{solution}
    In the answer we let the computer do all the work.  

\begin{pyblock}
I = range(0, len(a))
s = sorted(a)
plt.plot(I, s)
plt.show()
\end{pyblock}
  \end{solution}
\end{exercise}

\begin{exercise}
  Find now a way to invert $i\to s_i$, normalize the function to get a distribution, and make a new plot. 
  \begin{solution}
Here is one way. Note that we already imported matplotlib, so we don't have to that again.
\begin{pyblock}
def cdf(a):  
    y = range(1, len(a)+1)
    y = [yy/len(a) for yy in y] # normalize
    x = sorted(a)
    return x, y

x, y = cdf(a)

plt.plot(x, y)
plt.show()
\end{pyblock}


  \end{solution}
\end{exercise}

\begin{exercise}
In the previous exercise (read the solution), we start $y$ with 1 and end with \texttt{len(a)+1}. Why is that? 
  \begin{solution}
    The reason is that at $s_1$ the first observation occurs. Hence, $F$ should make a jump of at least one at $s_1$. Next, the \texttt{range} function works up to, but not including, its second argument. Hence (in code), \texttt{range(10)[-1]/10 = 0.9}, that is, the last element \texttt{range(10)[-]} of the set of numbers $0, 1, \ldots, 9$ is not 10. Hence, when we extend the range to \texttt{len(a)+1} we have a range up to and including the element we want to include.
  \end{solution}
\end{exercise}

You should know that for loops in python are quite slow (and for loops in R seem to be really dramatic). For large amounts of data it is better to use \texttt{numpy}. 


\begin{exercise}\label{ex:1}
  Use the numpy functions \texttt{arange}, to replace the \texttt{range}, and \texttt{sort} to speed up the algorithm of the previous exercise. 
  \begin{solution}
    This code is much, much faster, and also very clean. Note that we normalize \texttt{y} right away. 
\begin{pyblock}
def cdf(a):
    y = np.arange(1, len(a)+1)/len(a)
    x = np.sort(a)
    return x, y
  
\end{pyblock}
  \end{solution}
\end{exercise}


With the algorithm of Exercise~\ref{ex:1} we can  compute and plot a distribution function of interarrival times specified by a list (vector, array) $a$. For our present goals this suffices. If, however, you like details, you should notice that our plot of the distribution function is still not entirely OK:  the graph should make  jumps, but it doesn't.  Moreover, our cdf is not a real function, it can be of the form $x=(1,1,3)$, $y=(0, 0.5, 1)$. In the rest of this subsection we repair these points.
You can skip this if you are not interested. 

\begin{exercise}
Read about the \pyv{drawstyle} option of the plot function of matplotlib to see how to make jumps.
  \begin{solution}
With the drawstyle option: 
\begin{pyblock}
plt.plot(x, y,  drawstyle = 'steps-post')
plt.show()
\end{pyblock}


But now we still have vertical lines. To remove those, we can use \pyv{hlines}.

\begin{pyblock}
y = range(0, len(a)+1)
y = [yy/len(a) for yy in y] # normalize
s = sorted(a)
left = [min(a)-1] + a
right = a + [max(a)+1]

plt.hlines(y, left, right)
plt.show()
\end{pyblock}

There  we are!.
  \end{solution}
\end{exercise}


\begin{exercise}
Finally, we can make the computation of the cdf significantly faster with using the following numpy functions. 
\begin{enumerate}
\item \pyv{numpy.unique}
\item \pyv{numpy.sort}
\item \pyv{numpy.cumsum}
\item \pyv{numpy.sum}
\end{enumerate}
How can you use these to compute the cdf?
\begin{solution}
Here it is.
\begin{pyblock}

def cdf(X):
    # remove multiple occurences of the same value
    unique, count = np.unique(np.sort(X), return_counts=True)
    x = unique
    y = count.cumsum()/count.sum()
    return x, y

x, y = cdf(a)
\end{pyblock}

\end{solution}
\end{exercise}

\subsection{Simulating the arrival process of a single customer}
\label{sec:simulations}

The next step is to simulate interarrival times of a single customer and  make an empirical cdf of these times.  Then we graphically compare this cdf to the exponential distribution. A more formal method to compare cdfs is by means of the Kolmogorov-Smirnov statistic (see wikipedia) which we develop in passing.

\begin{exercise}
  Generate 3 random numbers uniformly distributed on $[4,6]$.  Print these numbers to see if you get something decent. Read the documentation of
 \pyv{scipy.stats.uniform}\footnote{When coding you should develop the habit to look up things on the web}.  Check in particular the \pyv{rvs()} function. 

\begin{solution}
Copy this code and run it.
\begin{pyverbatim}
from scipy.stats import uniform

# fix the seed
scipy.random.seed(3) 

# parameters
L = 3  # number of interarrival times

G = uniform(loc=4, scale=2) # G is called a frozen distribution.
a = G.rvs(L)
print(a)
\end{pyverbatim}
  
\end{solution}

\end{exercise}


\begin{exercise}
Generate $L=300$ random numbers $\sim U[4,6]$ and make a histogram of these numbers. You should interpret these random numbers as interarrival times of one customer at a shop (in hours say.)
\begin{solution}
Add this code to the other code and run it.
\begin{pyverbatim}
N = 1 # number of customers
L = 300
a = G.rvs(L)

plt.hist(a, bins=L/20, label="a")
plt.title("N = {}, L = {}".format(N, L))
plt.legend()
plt.show()
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
Compute  the empirical distribution function of these interarrival times, and plot the cdf.
\begin{solution}
Add this to the other code and run it.
\begin{pyverbatim}
x, y = cdf(a)
plt.plot(x,y,  label="d")
plt.legend()
plt.show()
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
We would like to numerically compare the empirical distribution of the interarrival tiems to the theoretical distribution, which is uniform in this case. 
For this we can use the Kolmogorov-Smirnov statistic. Try to come up with a method to compute this statistic, and then compute it. 

You might find the following functions helpful (read the documentation on the web to see what they do):
\begin{enumerate}
\item \pyv{numpy.max}
\item \pyv{numpy.abs}
\item \pyv{scipy.stats.uniform}, the \pyv{cdf} function.
\end{enumerate}

\begin{solution}
Add this to the other code and run it.
\begin{pyverbatim}
def KS(X, F):
    # Compute the Kolmogorov-Smirnov statistic where
    # X are the data points
    # F is the theoretical distribution
    support, y = cdf(X)
    y_theo = np.array([F.cdf(x) for x in support])
    return np.max(np.abs(y-y_theo))

print(KS(a, G))    
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  Now compute the KS statistics to compare the simulated interarrival times with an exponential distribution with a suitable mean. (What is this suitable mean?).

See \pyv{scipy.stats.expon}.


\begin{solution}
Add this to the other code and run it. Since the mean interarrival time is $5$, take $\lambda = 1/5$.

\begin{pyverbatim}
from scipy.stats import expon

labda = 1./5 # lambda is a function in python
E = expon(scale=1/labda) 
print(E.mean()) # to check that we chose the right scale
print(KS(a, E))    
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  Finally, plot the empirical distribution and the exponential distribution in one graph. Explain why these graphs are different.
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
x, y = cdf(a)
dist_name = "U[4,6]"
def plot_distributions(x, y, N, L, dist_name):
    # plot the empirical cdf and the theoretical cdf in one figure
    plt.title("X ~ {} with N = {}, L = {}".format(dist_name,N, L))
    plt.plot(x, y, label="empirical")
    plt.plot(x, E.cdf(x), label="exponential")
    plt.legend()
    plt.show()

plot_distributions(x, y, N, L, dist_name)	
\end{pyverbatim}

It is pretty obvious why these graphs must be different: we compare a uniform and an exponential distribution. 
\end{solution}
\end{exercise}


\subsection{Simulating many customers}
\label{sec:simul-many-cust}

We would now like to simulate the interarrival process as seen by a shop that serves many customers. For ease, we call this the merged, or superposed, interarrival process. Again, this requires quite a bit of thought. Thus, we start with a numerical example with two customers, and organize all steps we make to compute the empirical distribution of the merged interarrival process. Then we make an algorithm, and scale up to many numbers. 

\begin{exercise}
  Suppose we have two customers with interarrival times $a=[4, 3, 1.2, 5]$ and $b=[2, 0.5, 9]$. Make, by hand, the empirical cdf of the merged process.
  \begin{hint}
Note that the shop sees the combined arrival process, so first find a way to merge the arrival times of the customers into one arrival process as observed by the shop. Then convert this into interrival times at the shop.
  \end{hint}
  \begin{solution}
    The following steps in code explain the logic.
    \begin{pyblock}
a=[4, 3, 1.2, 5]
b=[2, 0.5, 9]

def compute_arrivaltimes(a):
    A=[0]
    i = 1
    for x in a:
        A.append(A[i-1] + x)
        i += 1

    return A

A = compute_arrivaltimes(a)
B = compute_arrivaltimes(b)


times = [0] + sorted(A[1:] + B[1:]) 
print(times)

shop = []
for s, t in zip(times[:-1], times[1:]):
    shop.append(t - s)

print(shop)
    \end{pyblock}
  \end{solution}
\end{exercise}


\begin{exercise}
  The steps of the previous exercise can be summarized by this code:
\begin{pyverbatim}
from itertools import chain

def superposition(a):
    A = np.cumsum(a, axis=1)
    A = list(sorted(chain.from_iterable(A)))
    return np.diff(A)

\end{pyverbatim}  

Try to understand this by reading the documentation (on the web) of the following functions.
\begin{enumerate}
\item \pyv{numpy.cumsum}, in particular read about the meaning of axis.
\item \pyv{itertools.chain.from_iterable}
\item \pyv{numpy.diff}
\end{enumerate}
\end{exercise}


\begin{exercise}
  Generate 100 random interarrival times for 3 customers, plot the cdf of the merged process, and compare to the exponential distribution with the correct mean. Also compute the Kolmogorov-Smirnov statistic for this case. What is the effect of increasing the number of customers from $N=1$ to $N=3$?
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
N, L = 3, 100
a = superposition(G.rvs((N, L)))

E = expon(scale=1./(N*labda))
print(E.mean())

x, y = cdf(a)
dist_name ="U[4,6]"
plot_distributions(x, y, N, L, dist_name)

print(KS(a, E)) # Compute KS statistic using the function defined earlier
\end{pyverbatim}

\end{solution}
\end{exercise}


\begin{exercise}
  Compare  the empirical distribution of the interarrival times generated by  $N=10$ customers to the exponential distribution (compute the appropriate arrival rate). Make a plot, and explain what you see.
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
N, L = 10, 100
a = superposition(G.rvs((N, L)))

E = expon(scale=1./(N*labda))

x, y = cdf(a)
dist_name ="U[4,6]"
plot_distributions(x, y, N, L, dist_name)

print(KS(a, E)) 
\end{pyverbatim}

This is great. For just $N=10$ we see that the exponential distribution is a real good fit. 
\end{solution}
\end{exercise}



\begin{exercise}
Do the same for $N=10$ customers with normally distributed interarrival times with $\mu=5$, and $\sigma =1$.
For this use \pyv{scipy.stats.norm}. What do you see? What is the influence of the distribution of interarrival times of an individual customer? 
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
from scipy.stats import norm

N, L = 10, 100

N_dist = norm(loc=5, scale=1)
a = superposition(N_dist.rvs((N, L)))
x, y = cdf(a)
dist_name = "N(5,1)"
plot_distributions(x, y, N, L, dist_name)

print(KS(a, E))
\end{pyverbatim}

Clearly, whether the distribution of interarrival times of an individual customer are uniform, or normal, doesn't really matter. In both cases the exponential distribution is a good model for what the shop sees. We did not analyze what happens if we would merge customers with normal distribution and uniform distribution, but we can suspect that in all these cases the merged process converges to a set of i.i.d. exponentially distributed random variables.

\end{solution}
\end{exercise}

\begin{exercise}
If  $\mu=\sigma=5$ then the analysis should break down. What happens if you set $\sigma=5$? If you don't get real strange results, the code itself must be wrong\footnote{When testing code it is also a good idea to see what happens if you use bogus numbers. The program should fail or give very strange results.}.
\begin{solution}
  Since $\sigma=\mu=5$, about $15\%$ of the `interarrival' times should be negative. This is clearly impossible. 
\end{solution}	
\end{exercise}

\begin{exercise}
  Make a summary of what you have learned from this tutorial.
  \begin{solution}
    Here are some ideas you should have learned. 
    \begin{enumerate}
    \item Algorithmic thinking, i.e., how to chop up a computational challenge into small steps.
    \item How to efficiently compute the empirical distribution function
    \item The Kolmogorov-Smirnov statistic
    \item The empirical distribution of a  merged interarrival arrival process converges, typically, super rapidly to an exponential distribution. Thus,  interarrival times at shops, hospitals and so on, are often very well described by an exponential distribution with suitable mean. 
    \item This convergence is not so sensitive to the distribution of the interarrival times of a single customers. For the shop, only the population matters. 
    \item Using functions (e.g., \pyv{def compute(a)})  to document code  (by the function name), hide complexity, and reuse code so that it can be applied multiple times. Moreover, defining functions is in line with the extremely important Dont-Repeat-Yourself (DRY) principle. 
    \item Coding skills: python, numpy and scipy.
    \end{enumerate}
  \end{solution}
\end{exercise}


%\subsection{Memoryless property}

% It is well known that the exponential distribution has the memoryless property. Does it hold for simulations too? 

% First generate our standard case.

% \begin{pyblock}
% N = 10
% runlength = 1000
% labda = N/m
% d = m
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% a = superposition(G, N, runlength)
% \end{pyblock} 

% Now select the interarrival times that exceed some number $x$ and
% subtract $x$. These new values should, hopefully, have the same
% distribution as the initial set of interarrival times.

% \begin{pyblock}
% x = 0.5 # threshold
% c = a[a>x] -x # select the interarrival times longer than x
% \end{pyblock} 

% Let's plot it
% \begin{pyblock}
% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% This is quite ok. What is the threshold $x$ becomes larger, e.g., 1?

% \begin{pyblock}
% x = 2. # threshold
% c = a[a>x] -x # select the interarrival times longer than x

% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% Now the result is not as nice. What if we increase the simulation length? 
% \begin{pyblock}
% N = 10
% runlength = 100000
% labda = N/m
% d = m
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% a = superposition(G, N, runlength)

% x = 2. # threshold
% c = a[a>x] -x # select the interarrival times longer than x

% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% It is interesting to see that now the memoryless property seems to
% hold again. But why do we need so many values to see this? 



% \subsection{Analysis of the number arrivals in an interval}

% Now I build the same data, but count the number of arrivals that occur in a certain interval. 

% First I compute, as above, a number of arrival times as seen by the counter.

% \begin{pyblock}
% m = 5.
% d = 2.
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% N = 300
% runlength = 1000
% a = G.rvs((N, runlength))
% A = np.cumsum(a, axis = 1)
% A = np.sort(A.flatten())
% \end{pyblock} 

% Next, I make a histogram, that, is I chop up the entire simulation
% interval, from the first arrival time to the last, into a number of
% bins of equal length. Then I use $np.histogram$ to count the number of
% arrivals in each such interval.  There are $N*runlength$ arrivals in
% total.  I like the bins of such size that on average each bin will
% contain 50 arrivals. Thus, the number of bins, i.e., the number of intervals in which the entire simulation interval needs to chopped, should be $N*runlength/50$. 

% \begin{pyblock}
% #bins = N*runlength/50
% #p, x = np.histogram(A, bins = bins)
% \end{pyblock} 

% Here, $x$ contains the interval boundaries in which the simulation
% interval is chopped up. Therefore $I=x[1]-x[0]$ is the length of one
% such interval. The arrival rate in such interval must be $I*N/m$, as
% $N/m$ is the arrival rate per unit time. Therefore,
% \begin{pyblock}
% #I = x[1]- x[0]
% #labda = float(I)*N/m
% \end{pyblock} 

% Now I count the number of times a certain number of arrivals occured.
% \begin{pyblock}
% P =  np.bincount(p)
% \end{pyblock} 

% Finally, I want to fit a Poisson distribution to see how well the
% Poisson distribution fits the data.  I need the support of the
% measured data, and store in $x$. As $np.bincount$ only counts, it is
% necessary to normalize $P$.
% \begin{pyblock}
% x = range(p.min(), p.max())

% plt.plot(P/float(sum(P)))
% plt.plot(x, scipy.stats.poisson.pmf(x, labda)) 
% plt.show()
% \end{pyblock} 

% Not bad. However, some experimentation shows that when $N$ is small,
% like 30 or so, and the $runlength$ is short, in the order of 100 or
% so, the quality of the Poisson approximation is much less. I do not
% fully understand why that is the case.

% \begin{pyblock}
% N = 30
% runlength = 300
% a = G.rvs((N, runlength))
% A = np.cumsum(a, axis = 1)
% A = np.sort(A.flatten())

% bins = N*runlength/50
% p, x = np.histogram(A, bins = bins)

% I = x[1]- x[0]
% labda = float(I)*N/m

% P =  np.bincount(p)
% plt.plot(P/float(sum(P)))

% x = range(p.min(), p.max())
% plt.plot(x, scipy.stats.poisson.pmf(x, labda)) 
% plt.show()
% \end{pyblock} 

\clearpage


\section{A single server queue}
\label{sec:single-server-queue}

In this tutorial we simulate the queueing behavior of a supermarkt or hospital, in fact, for a general service system. We first make a simple model of a queueing system, and then extend this to cover more and more difficult queueing situations. With these models we can provide insight into how to design or improve real-world queueing systems.  You will see, hopefully, how astonishingly easy it is with simulation to evaluate many types of decisions and design problems.  

For ease we  consider a queueing system in discrete time, and don't make a distinction between the number of jobs in the system and the number of jobs in queue. Thus, queue length corresponds here to all jobs in the system, c.f. Section 1.4 of the queueing book. 

\subsection{Set up}
\label{sec:set-up}


\begin{exercise}
Write down the recursions to compute the queue length at the end of a period based on the number of arrivals $a_i$ during  period $i$, the queue length $Q_i$ at the start, and the number of services $s_i$. Assume that service is provided at the start of the period.  

Use a for-loop to sketch an algorithm (in pseudo code). Then check the solution for the python code.

  \begin{solution}
    We need a few imports.
\begin{pyblock}
import numpy as np
import scipy
from scipy.stats import poisson
import matplotlib.pyplot as plt

scipy.random.seed(3) 


def compute_Q_d(a, s, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0 # starting level of the queue
    for i in range(1, len(a)):
        d[i] = min(Q[i-1], s[i])
        Q[i] = Q[i-1] + a[i] - d[i]

    return Q, d
  
\end{pyblock}

    One of the nicest things of python is that  the real code and pseudo code resemble each other so much.
  \end{solution}
  
\end{exercise}


With the code of the above exercises we can start our experiments.

\begin{exercise}
Copy the code of the previous exercise to a new file in Anaconda. Then add the code below and run it. Here $\lambda$ is the arrival rate, $\mu$ the service rate, $N$ the number of periods, and $q_0$ the starting level of the queue. Explain what the code does. Can you also explain the value of the mean and the standard deviation? 

  \begin{pyverbatim}

labda, mu, q0, N = 5, 6, 0, 100

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)
print(a.mean(), a.std())
\end{pyverbatim}
\begin{solution}
  Here is the complete code in case you have messed things up.

\begin{pyverbatim}
import numpy as np
import scipy
from scipy.stats import poisson
import matplotlib.pyplot as plt

scipy.random.seed(3) 


def compute_Q_d(a, s, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0 # starting level of the queue
    for i in range(1, len(a)):
        d[i] = min(Q[i-1], s[i])
        Q[i] = Q[i-1] + a[i] - d[i]

    return Q, d


labda, mu, q0, N = 5, 6, 0, 100
a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)
print(a.mean(), a.std())
\end{pyverbatim}

\end{solution}
\end{exercise}

\begin{exercise}
  Modify  the appropriate parts of  code of the previous exercise to the below,  and run it. Explain what you see.

  \begin{pyverbatim}
labda, mu, q0, N = 5, 6, 0, 100
a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)

plt.plot(Q)
plt.show()
  \end{pyverbatim}
\end{exercise}


\begin{exercise}
  Getting statistics is now really easy. For example,  sketch the empirical distribution function of the queue length process and Compute the mean number of departures per period.
  \begin{hint}
For  the empirical distribution function  you can use the code of Exercise~\ref{ex:1}. Before looking it up, try to recall how the cdf is computed.
  \end{hint}


  \begin{solution}
This it the code.    
  \begin{pyverbatim}
print(d.mean())
    
x, F = cdf(Q)
plt.plot(x, F)
plt.show()
  \end{pyverbatim}
  \end{solution}
\end{exercise}

\begin{exercise}
  Can you explain the value of the mean number of departures?
  \begin{solution}
    The mean number of departures must  be (about) equal  to the mean number arrivals per period. Jobs cannot enter from `nowhere'.
  \end{solution}
\end{exercise}


\begin{exercise}
Plot the queue length process for a large initial queue, for instance, with

\begin{pyverbatim}
q0, N = 1000, 100

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)

plt.plot(Q)
plt.show()
  \end{pyverbatim}

Next, set $q_0=10000$ and $N=1000$.  (In Anaconda you can just change the numbers and run the code again, in other words, you don't have to copy all the code.) Finally,  make these values again 10 times larger. 

Explain what you see. What is the drain rate of $Q$?
\begin{solution}
  The queue length drains at rate $\mu-\lambda$ when $q_0$ is really large. Thus, for such settings, you might just as well approximate the queue length behavior as $q(t) = q_0 - (\mu-\lambda)t$, i.e., as a deterministic system.
\end{solution}
\end{exercise}

\begin{exercise}
What do you expect to see when $\lambda=6$ and $\mu=5$? Once you formulated your hypothesis, check it.

\begin{pyverbatim}
N = 10000
labda = 6
mu = 5
q0 = 0

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)

plt.plot(Q)
plt.show()
  \end{pyverbatim}
Explain again what you see.
\end{exercise}

\subsection{What if analysis}
\label{sec:what-if-analysis}

Hopefully you  are  convinced by now about how powerful simulation is.  We can start asking all kinds of questions.

\begin{exercise}
  For instance,  the mean and the sigma of the queue length might be too large, that is, customers complain about long waiting times.  Suppose we are able, with significant technological investment, to make the service time more predictable. What would be the influence of this?

To quantify the effect of regularity of service times we first assume that the service times are exponentially distributed, then we change it to deterministic times.
 Deterministic service times are the best we can achieve. Thus, if we don't change the mean of the service times, it will not become any better than this.


So, let's test. First run this:
\begin{pyverbatim}
N = 10000
labda = 6
mu = 5
q0 = 0

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)
print(Q.mean(), Q.std())
\end{pyverbatim}
Then run the same code with \pyv{s = np.ones_like(a) * mu} rather than the Poisson distribution. Explain the result.
\begin{solution}
	You should observe that the variability of the queue length process decreases..
\end{solution}
\end{exercise}



\begin{exercise}
  Next, we might be able to reduce the average service time by 10\%, say, but reducing the variability is hard. To see the effect of this change, replace $s$ by
  \texttt{s = poisson(1.1*mu).rvs(N)}, i.e., we increases the service rate with $10\%$. 

Do the computations and compare the results to those of the previous exercise. What change in average service time do we need to get about the same average queue length as the one for the queueing system with deterministic service times? Is an $10\%$ increase enough, or should it be $20\%$, or $30\%$? Just test a few numbers and see what you get. 
  \begin{solution}
For our experiments it is about 20\% extra.
  \end{solution}
\end{exercise}

You should make the crucial observation now that we can experiment with all kinds of changes (system improvements) and compare their effects. In more general terms: simulation allows us to do `what-if' analysis. 

\subsection{Control }
\label{sec:control-}

In the previous section we analyzed the effect of the design of the system, such as changing the average service times. These changes are independent of the queue length. In many systems, however, the service rate depends on the dynamics of the queue process. When the queue is large, service rates increase, while when the queue is small, service rates decrease.  YOu see this type of policy often in supermarkets, extra cashiers will open when the queues increase.

\begin{exercise}
  Suppose that normally we have 6 servers, each working at rate $1$ per period. When the queue becomes longer than 20 we hire two extra servers, and when the queue is empty again, we send the extra two servers home, until the queue hits 20 again, and so on. Try to write your own code to compute the queue process. (This is pretty challenging, and a good test to see how creative you are in modelling.
  \begin{hint}
  As a hint, you need a state variable to track whether the extra servers are present or not. The solution shows the code.) Analyze the effect of the threshold at 20; what happens if you set it to 18, say, or 30? What is the effect of the number of extra servers; what if you would add 3 instead of 2?
  \end{hint}

  \begin{solution}
Here is one way.
    \begin{pyverbatim}
def compute_Q_d(a, q0=0, mu=6, threshold=20, extra=2):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0
    present = False # extra employees are not in
    for i in range(1, len(a)):
        rate = mu + extra if present else mu # service rate
        s = poisson(rate).rvs()
        d[i] = min(Q[i-1],s)
        Q[i] = Q[i-1] + a[i] - d[i]
        if Q[i] == 0:
            present = False # send employee home
        elif Q[i] >= threshold:
            present = True # hire employee for next period
    
    return Q, d
    
    \end{pyverbatim}


    Note that this code runs significantly  slower than the other code. This is because  we now  have to call \pyv{poisson(mu).rvs()} in every step of the for-loop. We could make the program run faster by using the scipy library to generate $N$ random numbers in one step outside of the for loop and then extract numbers from there when needed. However, for clarity we chose not to do so.
  \end{solution}
  
\end{exercise}


\begin{exercise}
Another way to deal with large queues is to simply  block customers if the queue is too long. What if we block at a level of 15? How would that affect the average queue and the distribution of the queue? 

  Modify the code to compute the queue and do an experiment.

\begin{solution}
Here is an example. What can be the meaning of \pyv{np.inf}? 

\begin{pyverbatim}
def compute_Q_d(a, s, q0=0, b=np.inf):
    # b is the blocking level.
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0
    for i in range(1, len(a)):
        d[i] = min(Q[i-1], s[i])
        Q[i] = min(b, Q[i-1] + a[i] - d[i])

    return Q, d


N = 10000
labda = 5
mu = 6
q0 = 0

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0, b=15)
print(Q.mean(), Q.std())

x, F = cdf(Q)
plt.plot(x, F)
plt.show()
\end{pyverbatim}

  \end{solution}
\end{exercise}

As a final case consider a single server queue that can be switched on and off. There is a cost $h$ associated with keeping a job waiting for one period, there is a cost $p$ to hire the server for one period, and it costs $s$ to switch on the server. Given the parameter values of the next exercise,  would what be a good threshold $N$ such that when the queue hits or exceeds $N$, the server is switched on? We assume (and it is easy to prove) that it is optimal to switch off the server when the queue is emtpy. 

\begin{exercise}
  Jobs arrive at rate $\lambda=0.3$ per period, if the server is present the service rate is $\mu=1$ per period. The number of arrivals and service are Poisson distributed with the given rates. Then, $h=1$ (without loss of generality), $p=5$ and $S=500$. Write a simulator to compute the average cost for setting $N=100$. Then, change $N$ to find a better value.

\begin{solution}
Here is all the code.  
\begin{pyverbatim}
num_jobs = 10000
labda = 0.3
mu = 1
q0 = 0
N = 100 # threshold

h = 1
p = 5
S = 500


def compute_cost(a, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0
    present = False  # extra employee is not in.
    queueing_cost = 0
    server_cost = 0
    setup_cost = 0
    for i in range(1, len(a)):
        if present:
            server_cost += p
            s = poisson(mu).rvs()
        else:
            s = 0
        d[i] = min(Q[i-1], s)
        Q[i] = Q[i-1] + a[i] - d[i]
        if Q[i] == 0:
            present = False  # send employee home
        elif Q[i] >= N:
            if present == False: 
	            present = True # server is switched on
                setup_cost += S            
        queueing_cost += h * Q[i]

    print(queueing_cost, setup_cost, server_cost)

    total_cost = queueing_cost + server_cost + setup_cost
    num_periods = len(a) - 1
    average_cost = total_cost / num_periods
    return average_cost

a = poisson(labda).rvs(num_jobs)
av = compute_cost(a, q0)
print(av)
\end{pyverbatim}
After a bit of experimentation, we see that $N=15$ is quite a bit better than $N=100$.

\end{solution}


\end{exercise}


\begin{exercise}
  What have you learned from this tutorial? What interesting extensions, relevant for practice,  can you thing of?
  \begin{solution}
    Some important points are the following.
    \begin{enumerate}
    \item  Making a  simulation requires some ingenuity, but is often not difficult
    \item With simulation it becomes possible to analyze many difficult queueing situations. The mathematical analysis of often much harder, if possible at all.
    \item We studied the behavior of queues under certain control policies, typically policies that change the service rate as a function of the queue length.
    \end{enumerate}

An interesting extension is to incorporate time-varying demand. In many systems, such as supermarkets, the demand is not constant over the day. In such cases the planning of servers should take this into account. 

  \end{solution}
\end{exercise}

\clearpage

\section{Simulation of the $G/G/c$ queue in continuous time}
\label{sec:ggc-continuous-time}

In this tutorial we will write a simulator for the $G/G/c$ queue. For this we use two concepts that are essential to simulate any stochastic system of reasonable complexity: an \emph{event stack} (or event schedule) to keep track of the sequence in which  events occur,  and \emph{classes} to organize data and behavior into single logical units.  We will work in steps  towards our goal; along the way you will learn a number of fundamental and highly interesting concepts such as \emph{classes}  and efficient \emph{data structures}.  If you have understood our implementation of the $G/G/c$ queue, discrete event simulators are no longer a black box for you. Hence, what you will learn from this practical extends well beyond the simulation of the $G/G/c$ queueing process. 


\subsection{Sorting with event stacks}
\label{sec:event-stacks}

If we simulate a stochastic system we like to move from event to event. To illustrate this idea, consider a  queueing process such as the $M/M/1$ queue.
It is clear that only at arrival and departure epochs something interesting happens; any time moment in between arrivals and departures can be neglected. Hence, in our simulator we prefer not to keep track of the entire time line, but just of the relevant epochs. If we can do this, we can jump from event to event.  

Clearly, we want to follow the sequence of events in the correct order of time. For this we use an \emph{event stack}.  To see how this works, it is best to consider a simple example first, and then extend to more difficult situations. 

Suppose we have 4 students, and we like to sort them in increasing order of age. One way to do this is to insert them into a list, but the insertion should respect the ordering right away. For this very general problem a number of efficient data structures have been developed, and one of these is the \emph{heap queue}.

\begin{exercise}
  Search the web on \texttt{python} and \texttt{heap queue}. Study the examples and write some code to sort the students Jan(21), Piet(20), Klara(18), Cythia(25).

  \begin{solution}
If you found, and read, the appropriate web page, you must have ended up with this code.
\begin{pyblock}
from heapq import heappop, heappush

stack = []

heappush(stack, (21, "Jan"))
heappush(stack, (20, "Piet"))
heappush(stack, (18, "Klara"))
heappush(stack, (25, "Cynthia"))

while stack:
    age, name = heappop(stack)
    print(name, age)

  \end{pyblock}

Pushing puts things in an sorted fashion on the stack, popping takes things from the stack. First we put the students on the stack. To print, we remove items from the stack until it is empty.
\end{solution}
  
\end{exercise}



\begin{exercise}
  Extend the code of the previous exercise such that we can include  with more information with the ages than just the name, for instance, also the brand of their mobile phone. 
\begin{solution}
We can make the  tuples, i.e., the data between the brackets,  just longer. 

\begin{pyblock}
from heapq import heappop, heappush
import numpy as np 
from scipy.stats import  expon

np.random.seed(3)

stack = []

heappush(stack, (21, "Jan", "Huawei"))
heappush(stack, (20, "Piet", "Apple"))
heappush(stack, (18, "Klara", "Motorola"))
heappush(stack, (25, "Cynthia", "Nexus"))

while stack:
    age, name, phone = heappop(stack)
    print(age, name, phone)

\end{pyblock}


\end{solution}
\end{exercise}


\begin{exercise}
  It may seem that we have just solved a simple toy problem, but this is not true. In fact, we have established something of real importance: heap queues form the core functionality of (nearly) all discrete event simulators used around the world.  To help you understand this fact, sketch how to use heap queues  to simulate a queueing process. 
  \begin{solution}
    For a queueing process we can start with putting a number of job arrivals on the stack and label these events as `arrivals'. Whenever a service starts, compute the departure time of a job, and put this departure moment on the stack. Label this event as a `departure'. Then move to the next event. Since, the event stack is sorted in time, the event at the head of the stack is the first moment in time something useful happens. 

More generally, a discrete-time stochastic process moves from event to event. At an event certain actions have to be taken, and these actions may involve the generation of new events or the removal of old events. The new events are put on the stack, the obsolete ones removed, and then the simulator moves to the first event on the stack. 
  \end{solution}
\end{exercise}



\subsection{G/G/1 queue}
\label{sec:gg1-queue}

In this section we will set up a simulator for the $G/G/1$ queue with  an event stack. Let us work in steps toward the final simulator.

We start with some basic imports,  initialize the stack,  and define two numbers \texttt{ARRIVAL} and \texttt{DEPARTURE} that will used to tag the type of event that will be popped from the event stack. So, make a new file, and the following at the  top of it.

\begin{pyblock}
from heapq import heappop, heappush
import numpy as np 
from scipy.stats import  expon

np.random.seed(3)

ARRIVAL = 0
DEPARTURE = 1

stack = [] # this is the event stack
\end{pyblock}


First of all we need jobs with arrival times and service times. The easiest way to handle jobs in a simulator is by means of a class, as in the code below.


\begin{pyblock}
class Job:
    def __init__(self):
        self.arrival_time = 0
        self.service_time = 0
        self.departure_time = 0
        self.queue_length_at_arrival = 0

    def sojourn_time(self):
        return self.departure_time - self.arrival_time

    def waiting_time(self):
        return self.sojourn_time() - self.service_time

    def __repr__(self):
        return f"{self.arrival_time}, {self.service_time}, {self.departure_time}\n"

  
\end{pyblock}

A class has a number of attributes to capture its  state and a number of functions\footnote{In python, functions related to a class are called `methods'. More generally, functions belonging to a class are called `member functions'.}. We initialize the arrival time and service time to zero. Then we store the departure time and queue length for a statistical analysis at the end of the simulations. Finally, we have functions to compute the waiting time and the sojourn time; the \texttt{repr} function is used to print the job. Note that our naming of functions also acts as documentation of what the functions do. Note also that member variables and functions in python start with the word \texttt{self}; this is to distinguish the (value of the) member variables from variables with the same name but lying outside the scope of the class.

Classes are extremely useful programming concepts, as it enables you to organize state (that is, attributes) and behavior (that is, functions that apply to the attributes) into logical components. Moreover, classes can offer functionality to a programmer without the programmer needing to understand how this functionality is built. Classes offer much more advantages such as inheritance, but we will not discuss that here. 

Once we have a class, making an object is simple with this code.\footnote{The wording is important here. Objects are instances of classes. As an example: Albert Einstein was a human being. Here, `Einstein' is the object, and `human being' is a class.}

\begin{pyblock}
job = Job()
job.arrival_time = 3
job.service_time = 2
\end{pyblock}

\begin{exercise}\label{ex:3}
  Make $10$ jobs with exponentially distributed interarrival times, with $\lambda=2$,  and exponentially distributed service times with $\mu=3$. Put these jobs on an event stack, and print them in order of arrival time. Tag the events with the job, and event type (which is an arrival).
  \begin{solution}
One way is like this.     
    \begin{pyblock}
labda = 2.
mu = 3.
rho = labda/mu
F = expon(scale=1./labda)  # interarrival time distributon
G = expon(scale=1./mu)  # service time distributon

num_jobs = 10

time = 0
for i in range(num_jobs):
    time += F.rvs()
    job = Job()
    job.arrival_time = time
    job.service_time = G.rvs()
    heappush(stack, (job.arrival_time, job, ARRIVAL))


while stack:
    time, job, typ = heappop(stack)
    print(job)
    
    \end{pyblock}
  \end{solution}
\end{exercise}

Now that we know how to make jobs and put them on an event stack, we can make a plan to simulate the $G/G/1$ queue. 

\begin{exercise}
  Replace the while loop of the previous exercise by the code below. Observe that we split events into two types\footnote{The word \texttt{type} is a reserved word in python, hence I use \texttt{typ} instead.}: arrivals and departures. 

  \begin{pyblock}
while stack:
    time, job, typ = heappop(stack)
    if typ == ARRIVAL:
        handle_arrival(time, job)
    else:
        handle_departure(time, job)
    
  \end{pyblock}

Now make a list of things that have to take place at an arrival event (in particular, what should happen when the server is busy at an arrival epoch, what should happen if the server is idle?) and at a departure event (in particular, what if the queue is empty, or not empty)? 

Turn your ideas into code and see whether you can get your simulator working. (Its not a problem if you spend some time on this; you will learn a lot in the process.)

\begin{solution}
We need a server object to keep track of the state of the server, and we also need  a list to queue the jobs. 

  \begin{pyblock}
class Server:
    def __init__(self):
        self.busy = False

server = Server()
queue = []
served_jobs = [] # used for statistics

def start_service(time, job):
    server.busy = True
    job.departure_time = time + job.service_time
    heappush(stack, (job.departure_time, job, DEPARTURE))

def handle_arrival(time, job):
    job.queue_length_at_arrival = len(queue)
    if server.busy:
        queue.append(job)
    else:
        start_service(time, job)
        
def handle_departure(time, job):
    server.busy = False
    if queue: # queue is not empty
        next_job = queue.pop(0) # get oldest job in queue
        start_service(time, next_job)
        
while stack:
    time, job, typ = heappop(stack)
    if typ == ARRIVAL:
        handle_arrival(time, job)
    else:
        handle_departure(time, job)
        served_jobs.append(job)

  \end{pyblock}
\end{solution}

\end{exercise}

\begin{exercise}
  Run a simulation for 100 jobs and compute the average queue length. Then extend to $1000$, and $10^5$. Compare it to the theoretical expected queue length of the $M/M/1$ queue at arrival moments. 
  \begin{solution}
Now we see that we needed to store the jobs in \texttt{served\_jobs}.

    \begin{pyblock}

tot_queue = sum(j.queue_length_at_arrival for j in served_jobs)
av_queue_length = tot_queue/len(served_jobs)
print(av_queue_length)
print('theoretical: ', rho*rho/(1-rho))
      
tot_sojourn = sum(j.sojourn_time() for j in served_jobs)
av_sojourn_length = tot_sojourn/len(served_jobs)
print(av_sojourn)
\end{pyblock}

You should see that you need a real large amount of jobs to obtain a good estimate for the (theoretical) expected queue length. I have to admit that I don't understand why (at least in my simulation, I need about $1e5$ jobs to get a good estimate. 

  \end{solution}
\end{exercise}

\begin{exercise}
  As a general observation, testing code is very important. Thus, let us get further confidence in our $G/G/1$ simulator by specializing it to the $D/D/1$ queue. Check the documentation of the \texttt{uniform} distribution in \texttt{scipy.stats} to see what the following code does
  \begin{pyblock}
from scipy.stats import uniform

F = uniform(3, 0.00001)
G = uniform(2, 0.00001)
\end{pyblock}

Then use this in our simulation. What happens? What happens if you reverse the 2 and the 3 in $F$ and $G$?
\begin{solution}
  With \texttt{F=uniform(3, 0.00001)} job interarrival times are nearly 3. Likewise, the service times are nearly 2. When you reverse the 2 and 3 a queue should build up. 
\end{solution}
\end{exercise}


\begin{exercise}
  Up to now we studied the $G/G/1$ FIFO queue. How to change the code to simulate a LIFO queue?
  \begin{solution}
    This is really easy: change the line \texttt{queue.pop(0)} by  \texttt{queue.pop()}. 
 \end{solution}
\end{exercise}

\begin{exercise}
  In a priority queue  jobs belong to a priority class. Jobs with higher priority are served before jobs with lower priority, and within one priority class jobs are served in FIFO sequence. A concrete example of a priority queue is the intake process of business and economy class customers at airports.  How would you build a $G/G/1$ priority queue?
  \begin{hint}
We have used a good data structure already. How should this be applied?
  \end{hint}
\begin{solution}
  As in the LIFO example, we only have to change the data structure. First, we give each job an extra attribute corresponding to its priority. Then we use a heap to store the jobs in queue. Specifically,  change the line with
\texttt{queue.append(job)} by \texttt{heappush(queue, (job.priority, job))}, and \texttt{queue.pop(0)} by \texttt{heappop(queue)}. 

\end{solution}
\end{exercise}

\begin{exercise}

  We implemented the queue as a python list. Why is a deque a more efficient data structure to simulate a queue?  Why is it important to know the efficiency of the data structures and algorithms you use?
  \begin{hint}
  Search the web on \texttt{python} and \texttt{deque}.
  \end{hint}
  \begin{solution}
    In a python list the \texttt{pop(0)} function is an $O(n)$ operation, where $n$ is the number of elements in the list. In a deque appending and removing items to either end of the deque is an $O(1)$ operation. 

When you do large scale simulations, involving many hours of simulation time, all inefficiencies build up and can make the running time orders of magnitude longer. A famous example is the sorting of numbers. A simple, but stupid, sorting algorithm is $O(n^2)$ while a good algorithm is $O(n \log n)$. The sorting time of $10^6$ numbers is dramatically different. 

  \end{solution}
\end{exercise}

\begin{exercise}
  Recall that in Exercise~\ref{ex:3} we build all jobs before the queueing simulation starts. Why is this not a good decision? Note that in the simulation of $G/G/c$ queue below we will generate jobs only when needed. 
  \begin{solution}
    Typically, generating all jobs at the start is not a real good idea. When running large simulations the amount of computer memory required to store all this data grows out of hand. Also our \texttt{served\_jobs} list is not memory efficient. 
  \end{solution}
\end{exercise}

\subsection{G/G/c queue}

\label{sec:ggc-queue}

Extending the simulation of the $G/G/1$ queue to a $G/G/c$ queue is not really hard. In the $G/G/1$ we have just one server which is busy or not.

\begin{exercise}
  Generalize  the \texttt{handle\_arrival} function of the $G/G/1$ queue such that it can cope with multiple servers.

  \begin{hint}
  Introduce a \texttt{num\_busy} variable that keeps track of the number of busy servers. What happens if a job arrives and this number is less than $c$; what if this number is equal to $c$?
  \end{hint}
  \begin{solution}
    If the number of busy servers is less than $c$, a service can start. Otherwise a job has to queue. If a service starts, increase \texttt{num\_busy} by one. 
  \end{solution}
\end{exercise}


\begin{exercise}
Likewise,   generalize  the \texttt{handle\_departure} function of the $G/G/1$ queue such that it can cope with multiple servers.
\begin{hint}
  Again, use the \texttt{num\_busy} variable. 
\end{hint}

\begin{solution}
  At a departure, a server becomes free, hence decrease \texttt{num\_busy} by one. If there are jobs in queue, start a new service.
\end{solution}
\end{exercise}

\begin{exercise}
  Can you think of some sort of property of the $G/G/c$ queue that must be true at all times? It is important to use such properties  as tests while developing.
  \begin{hint}
    There must be a relation between the queue length and the number of busy servers.
  \end{hint}
  \begin{solution}
    Of course, the number of busy servers cannot be negative or larger than $c$. The queue length cannot be negative. Finally, it cannot be that  the queue length is positive and the number of busy servers is less than $c$. 
  \end{solution}
\end{exercise}



\begin{hint}
  The design of the simulation of $G/G/1$ queue is not entirely satisfactory. For instance, if we want to run different experiments, we have to clean up old experiments before we can start new ones. The current design is not very practical for this. How to solve this?
  \begin{solution}
     Build the simulator as a class!
  \end{solution}
\end{hint}
\begin{solution}
Here is the entire class. Read it carefully.   


\begin{pyblock}
class GGc:
    def __init__(self, F, G, c, run_length):
        self.F = F
        self.G = G
        self.c = c
        self.num_busy = 0   # number of busy servers
        self.run_length = run_length
        self.stack = [] # event stack
        self.queue = deque()
        self.served_jobs = set()

    def handle_arrival(self, time, job):
        self.generate_new_arrival(time)
        job.queue_length_at_arrival = len(self.queue)
        if self.num_busy < self.c:
            self.start_service(time, job)
        else:
            self.queue.append(job)

    def generate_new_arrival(self, time):
        if self.run_length > 0:
            self.put_new_arrival_on_stack(time)
            self.run_length -= 1

    def put_new_arrival_on_stack(self, time):
        job = Job()
        job.arrival_time = time + self.F.rvs()  # new arrival time
        job.service_time = self.G.rvs()  # new service time
        heappush(self.stack, (job.arrival_time, job, ARRIVAL))

    def start_service(self, time, job):
        self.num_busy += 1  # server becomes busy.
        job.departure_time = time + job.service_time
        heappush(self.stack, (job.departure_time, job, DEPARTURE))

    def handle_departure(self, time, job):
        self.num_busy -= 1
        self.served_jobs.add(job)
        if self.queue:  # not empty
            next_job = self.queue.pop(0)
            self.start_service(time, next_job)

    def consistency_check(self):
        if ( self.num_busy < 0 or self.num_busy > self.c \
            or len(self.queue) < 0 or \
            (len(self.queue) > 0 and self.num_busy < self.c) ):
            print("there is something wrong")
            quit()

    def run(self):
        time = 0
        self.put_new_arrival_on_stack(time)

        while self.stack:  # not empty
            time, job, typ = heappop(self.stack)
            # self.consistency_check() # only use when testing.
            if typ == ARRIVAL:
                self.handle_arrival(time, job)
            else:
                self.handle_departure(time, job)


\end{pyblock}
\end{solution}

      

\begin{exercise}

How can we instantiate the \texttt{GGc} simulator, i.e., make a object of a class? How can we run a simulation with exponential interarrival times with $\lambda=2$, exponential service times with $\mu=1$, $c=3$, and $10$ jobs?
\begin{solution}
First we instantiate, then we run and print. 
\begin{pyblock}
ggc = GGc(expon(2), expon(1), 3, 10)
ggc.run()

print(ggc.served_jobs)
\end{pyblock}
\end{solution}
\end{exercise}

\begin{exercise}
Test the simulator with deterministic interarrival times, for instance a job that arrives every 10 minutes, and each job takes 1 hour of service. Check the departure process for $c=1$, $c=2$, $c=5$, $c=6$, and $c=7$. 
\end{exercise}


\begin{exercise}
  What other tests can you think of?
  \begin{solution}
    Implement the $M/M/3$ queue for instance, and compare the theoretical results with the simulation.  We can also check the quality of Sakasegawa's approximation. 
  \end{solution}
\end{exercise}

\begin{exercise}
  What have you learned in this tutorial, and how can you extend this work to more general cases? 
  \begin{solution}
    Topics learned.
    \begin{enumerate}
    \item Efficient data structures such as heap queues
    \item Event stacks to organize the tracking of events in time. With the concept of event stack, you now know discrete event simulators work. 
    \item The simulation of the $G/G/1$ queue and $G/G/c$ queue
    \item Python classes and, a bit more generally, object oriented programming. 
    \end{enumerate}

Some  interesting extensions.
    \begin{enumerate}
    \item   Scale up to networks of $G/G/c$ queues.
    \item In the $G/G/c$ queue the assumption is that all servers have the same service rate. In production settings this is typically not the case. There is a queue of jobs, and these jobs are served by machines with different speeds. For instance, old machines may work at a slower rate than new machines. 
    \item Above we also mentioned how to deal with priorities. Once we included priorities we can simulate the queueing behavior at the check-in desks at airports. 
    \end{enumerate}
  \end{solution}
\end{exercise}

\section{Queueing networks and workload control}
\label{sec:queueing-network}


\Closesolutionfile{hint}
\Closesolutionfile{ans}


\opt{solutionfiles}{
\clearpage
\section{Hints}
\input{hint}
\clearpage
\section{Solutions}
\input{ans}
}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
