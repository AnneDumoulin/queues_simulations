
\documentclass{scrartcl}
%\documentclass[a5paper]{article}

% als je de antwoorden aan het eind wil hebben, gebruik dan deze regel

%\usepackage[solutionfiles]{optional}

% Als je daarentegen de antwoorden onder de opdracht wil hebben, quote
% dan bovenstaande uit, en gebruik de regel hieronder
\usepackage[nosolutionfiles]{optional}

\opt{nosolutionfiles}{\usepackage[nosolutionfiles]{answers}}
\opt{solutionfiles}{\usepackage{answers}}

\usepackage{preamble}

% handle indentation below figure captions
\setcapwidth[c]{.8\textwidth}
\setcapwidth{0.95\textwidth}
%\setcapindent{0pt}
%\addtokomafont{caption}{\centering}

%\usepackage[top=5mm, left=5mm, right=5mm, bottom=2cm]{geometry}

\newenvironment{pyconcodeblock}%
 {\VerbatimEnvironment
  \begin{VerbatimOut}{temp.py}}%
 {\end{VerbatimOut}%
  \pyconc{exec(compile(open('temp.py', 'rb').read(), 'temp.py', 'exec'))}%
  \inputpygments{python}{temp.py}}





\title{Queueing simulations}
\author{N.D. Van Foreest and E.R. Van Beesten}
\begin{document}
\maketitle

\begin{itemize}
\item document in de levelcrossing illustration directory. 
\item Kijk even goed wat ik eigenlijk allemaal al heb.
\item Overshoot verdeling? 
\end{itemize}

\Opensolutionfile{ans}

\section{Introduction}

Typically a queueing system is subject to rules about when to allow jobs to enter the system or to adapt the service capacity. Such a decision rule is called a \emph{policy}.  The theoretical analysis of the efficacy of policies is often very hard, while with simulation it becomes doable.  In this document we present a number of cases to see how simulation can be used to analyze and improve queueing systems. Besides the fact that these the cases will improve your understanding of queueing systems and probability theory, they will also make clear  that simulation is a really creative activity and involves solving many interesting and challenging  algorithmic problems.  


Each case is organized in a number of exercises. For each exercise,
\begin{enumerate}
\item Make a design of how you want to solve the problem. For instance, make a model of a queueing system, or a control policy structure, or compute relevant KPIs (key performance indicators, such as cost, or utilization of the server, and so on). In other words, think before you type. 
\item Try to translate your ideas into pseudo code or, better yet,  python\footnote{Some of you might wonder why we use python rather than R. There are a few reasons for this. Python is more or less the third most used programming language, after c++ and java. It is widely used by companies, while R is hardly  used outside of academia. Programming OR applications is easier in python; it will also used in other courses. Finally, if you are interested in machine learning and artificial intelligence python is, hands-down, the best choice.}
  \item If you don't succeed in getting your program to work,  look up the code written by us and type it into your python environment.\footnote{Typing yourself forces you to read the code well.}.
  \item Simulate a number of scenarios by varying parameter settings and see what happens.
\end{enumerate}

We expect you to work in a groups of 2 to 3 students and bring a laptop with an \emph{installed and working} python environment, preferably  the anaconda package available at \url{https://www.anaconda.com/}, as this contains all functionality we will need\footnote{There are also python environments available on the web, such as repl.it., but that is typically a bit less practical than running the code on your own machine.}. You can find a nice tutorial to python programming at  \url{https://www.programiz.com/python-programming}. Note, this site advises to install python just by itself. We instead advise you to download anaconda, as this contains also the required numerical libraries. 

We will use the following libraries of python a lot:
\begin{itemize}
\item \pyv{numpy}  provides an enormous amount of functions to handle large (multi-dimensional) arrays with numbers. 
\item \pyv{scipy} contains numerical recipes, such as solvers for optimization software, solvers for differential equations. \pyv{scipy.stats} contains many probability distributions and numerical methods to operate on these functions. 
\item \pyv{matplotlib} provides plotting functionality.
\end{itemize}

Our code is not the most efficient, or fast. Rather, we focus on clarity of code so that the underlying reasoning is as clear as possible. Once our ideas and code are correct, we can start optimizing, if this is necessary. 

Finally, the code is part of the course, hence of the midterms and the exams.  Unless indicated as not obligatory, you have to be able to read the code and understand it.


\clearpage
\section{Exponential distribution}

The aim of this tutorial is to show, empirically, a fascination fact: even for very small populations in which individuals decide independently to visit a server (a shop, a hospital, etc),  the  exponential distribution is a good model for the interarrival times as seen by the server.  We will develop a simulation to motivate this `fact of nature'.  In particular, our aim is to build analogues to Figure 1.1, 1.2., and 1.3 of the queueing book, but in terms of cdfs instead of pdfs. (Read the description that underlies these figures.)

One important step in this process is to compute the empirical distribution. As this is much more interesting (and challenging) than you might think\footnote{If you search the web, you will see that computing the empirical density function is even more challenging.}, we start with this. Once we can compute empirical distribution functions, we are in good shape to set up the rest of the simulation. 

\begin{exercise}
  Make a plan of the steps you have to carry out to make Figure 1.1. In the next set of exercises we'll carry out these steps. So please do not read on before having thought about this problem.

  Spend some 5 minutes to think about how to approach the problem and how to chop it up into simple steps. Then organize the steps into a logical sequence. Don't worry at first about how to convert your ideas into computer code. Coding is a separate activity.  (As a matter of fact, I always start with making a small plan on how to turn an idea into code, and I call this step `modeling'. Typicallly this is a creative step, and not easy.) 

  \begin{solution}
    \begin{enumerate}
    \item Generate realizations of a uniformly distributed random variable representing the interarrival times of one customer.
    \item Plot the interarrival times.
    \item Compute (empirical) distribution function of the simulated interarrival times.
    \item Plot the (empirical) distribution function.
    \item Generate realizations of uniformly distributed random variables representing the interarrival times of multiple customers, e.g., 3. 
    \item Compute the arrival times for each customer.
    \item Merge  the arrival times for all customers. This is the arrival process as seen by the shop.
    \item Compute the interarrival times as seen by the shop.
    \item Plot these interarrival times.
    \item Compare to the exponential distribution function with a suitable arrival rate $\lambda$. 
    \end{enumerate}
  \end{solution}
\end{exercise}

\subsection{Empirical distributions}
\label{sec:empir-distr}

Before designing an algorithm, it is best to start with a simple numerical example and try to formalize the steps we take in the process.

\begin{exercise}
  Suppose you are given the following numbers: \pyb{a = [3.2, 4, 4, 1.3, 8.5, 9]}. What steps do you take to make the empirical distribution function? Recall, this is defined as
  \begin{equation}
    \label{eq:1}
    F(x) = \frac{\# \{i : a_i \leq x\}}{n}, 
  \end{equation}
  with $n$ is the length of $a$.

Can you turn it into an algorithm? (Just attempt to design an algorithm. Even if you don't succeed, trying is important.)

  \begin{solution}
We put the algorithm in a function so that we can use it later.
    \begin{pyblock}
def cdf(a):
    a = sorted(a)
    m, M = int(min(a)), int(max(a))+1
    # Since we know that a is sorted, this next line 
    # would be better, but less clear perhaps: 
    # m, M = int(a[0]), int(a[-1])+1 

    F = dict() # store the function i \to F[i]
    F[m-1]=0  # since F[x] = 0 for all x < m
    i = 0
    for x in range(m, M):
        F[x] = F[x-1]
        while i< len(a) and a[i] <= x:
            F[x] += 1
            i += 1

    # normalize
    for x in F.keys(): 
        F[x] /= len(a)

    return F
    \end{pyblock}

If we run this
\begin{pyblock}
F = cdf(a)
print(F)
\end{pyblock}
we get

\printpythontex

  \end{solution}
\end{exercise}

\begin{exercise}
  The method provided by the (solution of the) previous exercise is simple, but not completely correct. What is wrong?
  \begin{solution}
    We have to guess the support of $F$ (the set of points where $F$ makes the jumps) upfront, and we concentrated the support on the integers. However $F$ makes jumps at floats, for instance  at $3.2$. 
  \end{solution}
\end{exercise}

A better idea is to consider the sorted version of $a$. 

\begin{exercise}
  Sort the numbers in the list $a$, and make a plot (by hand) of $a$ as a function of the index, that is, plot the function $i\to \text{sorted}(a_i)$, with $i$ on the $x$-axis, and $a_i$ on the $y$-axis.  Observe this is the inverse of $F$ (except for the normalization).
  \begin{solution}
    In the answer we let the computer do all the work.  

\begin{pyblock}
# we first need to fix some initial settings.
import numpy as np
import matplotlib
matplotlib.use('pdf') 
import matplotlib.pyplot as plt

I = range(0, len(a))
plt.plot(I, sorted(a))
plt.show()
\end{pyblock}
  \end{solution}
\end{exercise}

\begin{exercise}
  Find now a way to invert $i\to a_i$, normalize the function to get a distribution, and make a new plot. 
  \begin{solution}
Here is one way. Note that we already imported matplotlib, so we don't have to that again.
\begin{pyblock}
def cdf(a):  
    y = range(1, len(a)+1)
    y = [yy/len(a) for yy in y] # normalize
    x = sorted(a)
    return x, y

x, y = cdf(a)

plt.plot(x, y)
plt.show()
\end{pyblock}
Why do we start $y$ with 1?

  \end{solution}
\end{exercise}

We now compute and plot a distribution function of interarrival times specified by a list (vector, array) $a$. For our present goals this suffices. If, however, you like details, you should notice that our plot of the distribution function is still not entirely OK:  the graph should make  jumps, but it doesn't.  In the rest of this section we show how to repair this, and we discuss a faster way to compute the cdf. You can skip this if you are not interested. 

\begin{exercise}
Read about the \pyv{drawstyle} option of the plot function of matplotlib to see how to make jumps.
  \begin{solution}
With the drawstyle option: 
\begin{pyblock}
plt.plot(x, y,  drawstyle = 'steps-post')
plt.show()
\end{pyblock}


But now we still have vertical lines. To remove those, we can use \pyv{hlines}.

\begin{pyblock}
y = range(0, len(a)+1)
y = [yy/len(a) for yy in y] # normalize
a = sorted(a)
left = [min(a)-1] + a
right = a + [max(a)+1]

plt.hlines(y, left, right)
plt.show()
\end{pyblock}

There  we are!.
  \end{solution}
\end{exercise}


\begin{exercise}
Finally, we can make the computation of the cdf significantly faster with using the following numpy functions. 
\begin{enumerate}
\item \pyv{numpy.unique}
\item \pyv{numpy.sort}
\item \pyv{numpy.cumsum}
\item \pyv{numpy.sum}
\end{enumerate}
How can you use these to compute the cdf?
\begin{solution}
Here it is.
\begin{pyblock}

def cdf(X):
    # remove multiple occurences of the same value
    unique, count = np.unique(np.sort(X), return_counts=True)
    x = unique
    y = count.cumsum()/count.sum()
    return x, y

x, y = cdf(a)
\end{pyblock}

\end{solution}
\end{exercise}

\subsection{Simulating the arrival process of a single customer}
\label{sec:simulations}

The next step is to simulate interarrival times of a single customer and  make an empirical cdf of these times.  Then we graphically compare this cdf to the exponential distribution. A more formal method to compare cdfs is by means of the Kolmogorov-Smirnov statistic (see wikipedia) which we develop in passing.

\begin{exercise}
  Generate 3 random numbers uniformly distributed on $[4,6]$.  Print these numbers to see if you get something decent. Read the documentation, on the web, of:
 \pyv{scipy.stats.uniform}.  Check in particular the \pyv{rvs()} function. 

\begin{solution}
Copy this code and run it.
\begin{pyverbatim}
# we first need to fix some initial settings.
import numpy as np
import scipy
import matplotlib
matplotlib.use('pdf') 
import matplotlib.pyplot as plt
from scipy.stats import uniform

np.set_printoptions(precision=3) # this is to print not too many digits to the screen

# fix the seed
scipy.random.seed(3) 

# parameters
L = 3  # number of interarrival times

G = uniform(loc=4, scale=2) # G is called a frozen distribution.
a = G.rvs(L)
print(a)
\end{pyverbatim}
  
\end{solution}

\end{exercise}


\begin{exercise}
Generate $L=300$ random numbers $\sim U[4,6]$ and make a histogram of these numbers. You should interpret these random numbers as interarrival times of one customer at a shop (in hours say.)
\begin{solution}
Add this code to the other code and run it.
\begin{pyverbatim}
N = 1 # number of customers
L = 300
a = G.rvs(L)

plt.hist(a, bins=L/20, label="a")
plt.title("N = {}, L = {}".format(N, L))
plt.legend()
plt.show()
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
Compute  the empirical distribution function of these interarrival times, and plot the cdf.
\begin{solution}
Add this to the other code and run it.
\begin{pyverbatim}
x, y = cdf(a)
plt.plot(x,y,  label="d")
plt.legend()
plt.show()
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
We would like to numerically compare the empirical distribution of the interarrival tiems to the theoretical distribution, which is uniform in this case. 
For this we can use the Kolmogorov-Smirnov statistic. Try to come up with a method to compute this statistic, and then compute it. 

You might find the following functions helpful (read the documentation on the web to see what they do):
\begin{enumerate}
\item \pyv{numpy.max}
\item \pyv{numpy.abs}
\item \pyv{scipy.stats.uniform}, the \pyv{cdf} function.
\end{enumerate}

\begin{solution}
Add this to the other code and run it.
\begin{pyverbatim}
def KS(X, F):
    # Compute the Kolmogorov-Smirnov statistic where
    # X are the data points
    # F is the theoretical distribution
    support, y = cdf(X)
    y_theo = np.array([F.cdf(x) for x in support])
    return np.max(np.abs(y-y_theo))

print(KS(a, G))    
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  Now compute the KS statistics to compare the simulated interarrival times with an exponential distribution with a suitable mean. (What is this suitable mean?).

See \pyv{scipy.stats.expon}.


\begin{solution}
Add this to the other code and run it. Since the mean interarrival time is $5$, take $\lambda = 1/5$.

\begin{pyverbatim}
from scipy.stats import expon

labda = 1./5 # lambda is a function in python
E = expon(scale=1/labda) 
print(E.mean()) # to check that we chose the right scale
print(KS(a, E))    
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  Finally, plot the empirical distribution and the exponential distribution in one graph. Explain why these graphs are different.
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
x, y = cdf(a)
dist_name = "U[4,6]"
def plot_distributions(x, y, N, L, dist_name):
    # plot the empirical cdf and the theoretical cdf in one figure
    plt.title("X ~ {} with N = {}, L = {}".format(dist_name,N, L))
    plt.plot(x, y, label="empirical")
    plt.plot(x, E.cdf(x), label="exponential")
    plt.legend()
    plt.show()

plot_distributions(x, y, N, L, dist_name)	
\end{pyverbatim}

It is pretty obvious why these graphs must be different: we compare a uniform and an exponential distribution. 
\end{solution}
\end{exercise}


\subsection{Simulating many customers}
\label{sec:simul-many-cust}

We would now like to simulate the interarrival process as seen by a shop that serves many customers. For ease, we call this the merged, or superposed, interarrival process. Again, this requires quite a bit of thought. Thus, we start with a numerical example with two customers, and trace and organize all steps we make to compute the empirical distribution of the merged interarrival process. Then we make an algorithm, and scale up to many numbers. 

\begin{exercise}
  Suppose we have two customers with interarrival times $a=[4, 3, 1.2, 5]$ and $b=[2, 0.5, 9]$. Make, by hand, the empirical cdf of the merged process.
(As a hint, note that the shop sees the combined arrival process, so first find a way to merge the arrival times of the customers into one arrival process as observed by the shop. Then convert this into interrival times at the shop.)
  \begin{solution}
    The following steps in code explain the logic.
    \begin{pyblock}
a=[4, 3, 1.2, 5]
b=[2, 0.5, 9]

def compute_arrivaltimes(a):
    A=[0]
    i = 1
    for x in a:
        A.append(A[i-1] + x)
        i += 1

    return A

A = compute_arrivaltimes(a)
B = compute_arrivaltimes(b)


times = [0] + sorted(A[1:] + B[1:]) 
print(times)

shop = []
for s, t in zip(times[:-1], times[1:]):
shop.append(t - s)

print(shop)
    \end{pyblock}
  \end{solution}
\end{exercise}


\begin{exercise}
  The steps of the previous exercise can be summarized by this code:
\begin{pyverbatim}
from itertools import chain

def superposition(a):
A = np.cumsum(a, axis=1)
A = list(sorted(chain.from_iterable(A)))
return np.diff(A)

\end{pyverbatim}  

Try to understand this by reading the documentation (on the web) of the following functions.
\begin{enumerate}
\item \pyv{numpy.cumsum}, in particular read about the meaning of axis.
\item \pyv{itertools.chain.from_iterable}
\item \pyv{numpy.diff}
\end{enumerate}
\end{exercise}


\begin{exercise}
  Generate 100 random interarrival times for 3 customers, plot the cdf of the merged process, and compare to the exponential distribution with the correct mean. Also compute the Kolmogorov-Smirnov statistic for this case. What is the effect of increasing the number of customers from $N=1$ to $N=3$?
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
N, L = 3, 100
a = superposition(G.rvs((N, L)))

E = expon(scale=1./(N*labda))
print(E.mean())

x, y = cdf(a)
dist_name ="U[4,6]"
plot_distributions(x, y, N, L, dist_name)

print(KS(a, E)) # Compute KS statistic using the function defined earlier
\end{pyverbatim}

\end{solution}
\end{exercise}


\begin{exercise}
  Compare  the empirical distribution of the interarrival times generated by  $N=10$ customers to the exponential distribution (compute the appropriate arrival rate). Make a plot, and explain what you see.
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
N, L = 10, 100
a = superposition(G.rvs((N, L)))

E = expon(scale=1./(N*labda))

x, y = cdf(a)
dist_name ="U[4,6]"
plot_distributions(x, y, N, L, dist_name)

print(KS(a, E)) 
\end{pyverbatim}

This is great. For just $N=10$ we see that the exponential distribution is a real good fit. 
\end{solution}
\end{exercise}



\begin{exercise}
Do the same for $N=10$ customers with normally distributed interarrival times with $\mu=5$, and $\sigma =1$.
For this use \pyv{scipy.stats.norm}. What do you see? What is the influence of the distribution of interarrival times of an individual customer? 
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
from scipy.stats import norm

N, L = 10, 100

N_dist = norm(loc=5, scale=1)
a = superposition(N_dist.rvs((N, L)))
x, y = cdf(a)
dist_name = "N(5,1)"
plot_distributions(x, y, N, L, dist_name)

print(KS(a, E))
\end{pyverbatim}

Clearly, whether the distribution of interarrival times of an individual customer are uniform, or normal, doesn't really matter. In both cases the exponential distribution is a good model for what the shop sees. We did not analyze what happens if we would merge customers with normal distribution and uniform distribution, but we can suspect that in all these cases the merged process converges to a set of i.i.d. exponentially distributed random variables.

\end{solution}
\end{exercise}

\begin{exercise}
If  $\mu=\sigma=5$ then the analysis should break down. What happens if you set $\sigma=5$? If you don't get real strange results, the code itself must be wrong. (As an aside, when testing code it is good to see what happens if you use bogus numbers. The program should fail.)
\begin{solution}
	SOLUTION WAS MISSING. PROBABLY YOU MEAN THAT THERE ARE NEGATIVE INTERARRIVAL TIMES NOW, WHICH DOES NOT MAKE SENSE. HOWEVER, WHEN I TRY IT OUT, THE CODE STILL RUNS SMOOTHLY AND THE RESULTS LOOK JUST FINE.
\end{solution}	
\end{exercise}

\begin{exercise}
  Make a summary of what you have learned from this tutorial.
  \begin{solution}
    \begin{enumerate}
    \item Algorithmic thinking, i.e., how to chop up a computational challenge into small steps.
    \item How to efficiently compute the empirical distribution function
    \item The Kolmogorov-Smirnov statistic
    \item The empirical distribution of a  merged interarrival arrival process converges, typically, super rapidly to an exponential distribution. Thus,  interarrival times at shops, hospitals and so on, are often very well described by an exponential distribution with suitable mean. 
    \item This convergence is not so sensitive to the distribution of the interarrival times of a single customers. For the shop, only the population matters. 
    \item Using functions (e.g., \pyv{def compute(a)})  to document code  (by the function name), hide complexity, and reuse code so that it can be applied multiple times. Moreover, defining functions is in line with the extremely important Dont-Repeat-Yourself (DRY) principle. 
    \item Python, numpy and scipy coding skills
    \end{enumerate}
  \end{solution}
\end{exercise}


%\subsection{Memoryless property}

% It is well known that the exponential distribution has the memoryless property. Does it hold for simulations too? 

% First generate our standard case.

% \begin{pyblock}
% N = 10
% runlength = 1000
% labda = N/m
% d = m
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% a = superposition(G, N, runlength)
% \end{pyblock} 

% Now select the interarrival times that exceed some number $x$ and
% subtract $x$. These new values should, hopefully, have the same
% distribution as the initial set of interarrival times.

% \begin{pyblock}
% x = 0.5 # threshold
% c = a[a>x] -x # select the interarrival times longer than x
% \end{pyblock} 

% Let's plot it
% \begin{pyblock}
% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% This is quite ok. What is the threshold $x$ becomes larger, e.g., 1?

% \begin{pyblock}
% x = 2. # threshold
% c = a[a>x] -x # select the interarrival times longer than x

% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% Now the result is not as nice. What if we increase the simulation length? 
% \begin{pyblock}
% N = 10
% runlength = 100000
% labda = N/m
% d = m
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% a = superposition(G, N, runlength)

% x = 2. # threshold
% c = a[a>x] -x # select the interarrival times longer than x

% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% It is interesting to see that now the memoryless property seems to
% hold again. But why do we need so many values to see this? 



% \subsection{Analysis of the number arrivals in an interval}

% Now I build the same data, but count the number of arrivals that occur in a certain interval. 

% First I compute, as above, a number of arrival times as seen by the counter.

% \begin{pyblock}
% m = 5.
% d = 2.
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% N = 300
% runlength = 1000
% a = G.rvs((N, runlength))
% A = np.cumsum(a, axis = 1)
% A = np.sort(A.flatten())
% \end{pyblock} 

% Next, I make a histogram, that, is I chop up the entire simulation
% interval, from the first arrival time to the last, into a number of
% bins of equal length. Then I use $np.histogram$ to count the number of
% arrivals in each such interval.  There are $N*runlength$ arrivals in
% total.  I like the bins of such size that on average each bin will
% contain 50 arrivals. Thus, the number of bins, i.e., the number of intervals in which the entire simulation interval needs to chopped, should be $N*runlength/50$. 

% \begin{pyblock}
% #bins = N*runlength/50
% #p, x = np.histogram(A, bins = bins)
% \end{pyblock} 

% Here, $x$ contains the interval boundaries in which the simulation
% interval is chopped up. Therefore $I=x[1]-x[0]$ is the length of one
% such interval. The arrival rate in such interval must be $I*N/m$, as
% $N/m$ is the arrival rate per unit time. Therefore,
% \begin{pyblock}
% #I = x[1]- x[0]
% #labda = float(I)*N/m
% \end{pyblock} 

% Now I count the number of times a certain number of arrivals occured.
% \begin{pyblock}
% P =  np.bincount(p)
% \end{pyblock} 

% Finally, I want to fit a Poisson distribution to see how well the
% Poisson distribution fits the data.  I need the support of the
% measured data, and store in $x$. As $np.bincount$ only counts, it is
% necessary to normalize $P$.
% \begin{pyblock}
% x = range(p.min(), p.max())

% plt.plot(P/float(sum(P)))
% plt.plot(x, scipy.stats.poisson.pmf(x, labda)) 
% plt.show()
% \end{pyblock} 

% Not bad. However, some experimentation shows that when $N$ is small,
% like 30 or so, and the $runlength$ is short, in the order of 100 or
% so, the quality of the Poisson approximation is much less. I do not
% fully understand why that is the case.

% \begin{pyblock}
% N = 30
% runlength = 300
% a = G.rvs((N, runlength))
% A = np.cumsum(a, axis = 1)
% A = np.sort(A.flatten())

% bins = N*runlength/50
% p, x = np.histogram(A, bins = bins)

% I = x[1]- x[0]
% labda = float(I)*N/m

% P =  np.bincount(p)
% plt.plot(P/float(sum(P)))

% x = range(p.min(), p.max())
% plt.plot(x, scipy.stats.poisson.pmf(x, labda)) 
% plt.show()
% \end{pyblock} 

\clearpage


\section{A single server queue}
\label{sec:single-server-queue}

In this tutorial we simulate the queueing behavior of a supermarkt or hospital, in fact, for a general service system. We first make a simple model of a queueing system, and then extend this to cover more and more difficult queueing situations. With these models we can provide insight into how to design or improve real-world queueing systems.  You will see, hopefully, how astonishingly easy it is to evaluate many types of decisions and design problems. 

For ease we  consider a queueing system in discrete time, and don't make a distinction between the number of jobs in the system and the number of jobs in queue. Thus, queue length corresponds here to all jobs in the system.

\subsection{Set up}
\label{sec:set-up}


\begin{exercise}
Write down the recursions to compute the queue length at the end of a period based on the number of arrivals $a_i$ during  period $i$, the queue length $Q_i$ at the start, and the number of services $s_i$. Assume that service is provided at the start of the period.  

Use a for-loop to sketch an algorithm (in pseudo code). Then check the solution for the python code.

  \begin{solution}
We need a few imports for later purposes.   One of the nicest things of python is to see how much the real code and pseudo code resemble each other.

\begin{pyblock}
import numpy as np
import scipy
from scipy.stats import poisson
import matplotlib.pyplot as plt

scipy.random.seed(3) 


def compute_Q_d(a, s, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0 # starting level of the queue
    for i in range(1, len(a)):
        d[i] = min(Q[i-1], s[i])
        Q[i] = Q[i-1] + a[i] - d[i]

    return Q, d
  
\end{pyblock}

  \end{solution}
  
\end{exercise}

\begin{exercise}
  Sketch (in pseudo code, or python) how to compute the empirical distribution function of a set of numbers $a$. (Resist the temptation to look it up from the previous tutorial. It is very important to actively try to recall what you learned, even if you don't succeed.)
  \begin{solution}
Here it is
\begin{pyblock}
def cdf(a):
    y = np.arange(1, len(a)+1)/len(a)
    x = np.sort(a)
    return x, y
  
\end{pyblock}
  \end{solution}
\end{exercise}

With the code of the above exercises we can start our experiments.

\begin{exercise}
Copy the code of the previous exercises to spyder, and run the code below. Here $\lambda$ is the arrival rate, $\mu$ the service rate, $N$ the number of periods, and $q_0$ the starting level of the queue. Explain what the code does. Can you also explain the value of the mean and the standard deviation? (Check the solution to see all necessary code.) 

  \begin{pyverbatim}
from scipy.stats import poisson
labda, mu, q0, N = 5, 6, 0, 100

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)
print(a.mean(), a.std())
\end{pyverbatim}
\begin{solution}
  Here is the complete code in case you have messed things up.

\begin{pyverbatim}
import numpy as np
import scipy
from scipy.stats import poisson
import matplotlib.pyplot as plt

scipy.random.seed(3) 


def compute_Q_d(a, s, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0 # starting level of the queue
    for i in range(1, len(a)):
        d[i] = min(Q[i-1], s[i])
        Q[i] = Q[i-1] + a[i] - d[i]

    return Q, d

def cdf(a):
    y = np.arange(1, len(a)+1)/len(a)
    x = np.sort(a)
    return x, y
  

labda, mu, q0, N = 5, 6, 0, 100
a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)
print(a.mean(), a.std())
\end{pyverbatim}

\end{solution}
\end{exercise}

\begin{exercise}
  Modify  the appropriate parts of  code of the previous exercise to the below,  and run it. Explain what you see.

  \begin{pyverbatim}
labda, mu, q0, N = 5, 6, 0, 100
a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)

plt.plot(Q)
plt.show()
  \end{pyverbatim}
\end{exercise}

\begin{exercise}
  Getting statistics is now really easy.
  \begin{pyverbatim}
print(Q.mean(), Q.std())
print(d.mean())
    
x, F = cdf(Q)
plt.plot(x, F)
plt.show()
  \end{pyverbatim}
Can you explain the value of the mean of the departure process $d$?
\end{exercise}

\begin{exercise}
Plot the queue length process for a large initial queue, for instance, with

\begin{pyverbatim}
q0, N = 1000, 100

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)

plt.plot(Q)
plt.show()
  \end{pyverbatim}

Next, set $q_0=10000$ and $N=1000$.  (In spyder you can just change the numbers and run the code again, in other words, you don't have to copy all the code.) Finally,  make these values again 10 times larger. 

Explain what you see. What is the drain rate of $Q$?
\begin{solution}
  The queue length drains at rate $\mu-\lambda$ when $q_0$ is really large. Thus, for such settings, you might just as well approximate the queue length behavior as $q(t) = q_0 - (\mu-\lambda)t$, i.e., as a deterministic system.
\end{solution}
\end{exercise}

\begin{exercise}
What do you expect to see when $\lambda=6$ and $\mu=5$? Once you formulated your hypothesis, check it.

\begin{pyverbatim}
N = 10000
labda = 6
mu = 5
q0 = 0

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)

plt.plot(Q)
plt.show()
  \end{pyverbatim}
Explain again what you see.
\end{exercise}

\begin{exercise}
What do you expect to see when we change the service distribution? What would it be if  it were constant,  for instance?

Change the code for the services into the following.
\begin{pyverbatim}
s = np.ones_like(a) * mu
\end{pyverbatim}
(What does this code do?) Explain again what you see. Why doesn't it make sense to plot the empirical distribution of $Q$ for this case?
\end{exercise}

\subsection{What if analysis}
\label{sec:what-if-analysis}

Hopefully you  are  convinced by now about how powerful simulation is.  We can start asking all kinds of questions.

\begin{exercise}
  For instance,  the mean and the sigma of the queue length might be too large, that is, customers complain about long waiting times.  Suppose we are able, with significant technological investment, to make the service time more predictable. What would be the influence of this?

To quantify the effect of regularity of service times we first assume that the service times are exponentially distributed, then we change it to deterministic times.
 Deterministic service times are the best we can achieve. Thus, if we don't change the mean of the service times, it will not become any better than this.


So, lets test. First run this:
\begin{pyverbatim}
N = 10000
labda = 6
mu = 5
q0 = 0

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)

Q, d = compute_Q_d(a, s, q0)
print(Q.mean(), Q.std())
  \end{pyverbatim}
  Then run the same code with \pyv{s = np.ones_like(a) * mu} rather than the Poisson distribution. Explain the result.
\end{exercise}



\begin{exercise}
  Next, we might be able to reduce the average service time by 10\%, say, but reducing the variability is hard, what should we take for $s$? Do the computations and compare the results to those of the previous exercise. What change in average time do we need to get about the same average queue length for the queueing system with deterministic service times?
  \begin{solution}
For our experiments it is about 20\% extra.
  \end{solution}
\end{exercise}



You should make the crucial observation now that we can experiment with all kinds of changes (system improvements) and compare their effects. In more general terms: simulation allows us to do `what-if' analysis. 




\subsection{Control }
\label{sec:control-}

In the previous section we analyzed the effect of the design of the system, such as changing the average service times. These changes are independent of the queue length. In many systems, however, the service rate depends on the dynamics of the queue process. When the queue is large, service rates increase, while when the queue is small, service rates decrease. 

\begin{exercise}
  Suppose that normally we have 6 servers, each working at rate $1$ per period. When the queue becomes longer than 10 we hire two extra servers, and when the queue is empty again, we send the extra two servers home, until the queue hits 20 again, and so on. Try to write your own code to compute the queue process. (This is pretty challenging, and a good test to see how creative you are in modelling. As a hint, you need a state variable to track whether the extra servers are present or not. The solution shows the code.) Analyze the effect of the threshold at 20; what happens if you set it to 18, say, or 30? What is the effect of the number of extra servers; what if you would add 3 instead of 2?
  

  \begin{solution}
    \begin{pyverbatim}
def compute_Q_d(a, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0
    present = False # extra employee is not in.
    for i in range(1, len(a)):
        mu = 8 if present else 6
        s = poisson(mu).rvs()
        d[i] = min(Q[i-1], s)
        Q[i] = Q[i-1] + a[i] - d[i]
        if Q[i] == 0:
            present = False # send employee home
        elif Q[i] >= 10:
            present = True # hire employee for next period

    return Q, d
    
    \end{pyverbatim}



Note that this code runs significantly slower than the other code. The reason that here we have to call \pyv{poisson(mu).rvs()} in every step of the for-loop. In the other code we could use the scipy library to generate $N$ random numbers in one step. (In you are interested, loops in C++ can be a factor 1000 or so faster than loops in python.)
  \end{solution}
  
\end{exercise}


\begin{exercise}
Another way to deal with large queues is to simply  block customers if the queue is too long (or customers don't want to join the queue if its too long). What if we block at a level of 15? How would that affect the average queue and the distribution of the queue? 

  Modify the code to compute the queue and do an experiment.

\begin{solution}
Here is an example. What can be the meaning of \pyv{np.inf}? 

\begin{pyverbatim}
def compute_Q_d(a, s, q0=0, b = np.inf):
    # b is the blocking level.
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0
    for i in range(1, len(a)):
        d[i] = min(Q[i-1], s[i])
        Q[i] = min(b, Q[i-1] + a[i] - d[i])

    return Q, d


N = 10000
labda = 5
mu = 6
q0 = 0

a = poisson(labda).rvs(N)
s = poisson(mu).rvs(N)
#s = np.ones_like(a) * mu

Q, d = compute_Q_d(a, s, q0, b=15)
print(Q.mean(), Q.std())

x, F = cdf(Q)
plt.plot(x, F)
plt.show()
\end{pyverbatim}

  \end{solution}
\end{exercise}

As a final case consider a single server queue that can be switched on and off. There is a cost $h$ associated with keeping a job waiting for one period, there is a cost $p$ to hire the server for one period, and it costs $s$ to switch on the server. Given the parameter values of the next exercise,  would what be a good threshold $N$ such that when the queue hits or exceeds $N$, the server is switched on. We assume (and it is easy to prove) that it is optimal to switch off the server when the queue is emtpy. 

\begin{exercise}
  Jobs arrive at rate $\lambda=0.3$ per period, if the server is present the service rate is $\mu=1$ per period. The number of arrivals and service are Poisson distributed with the given rates. Then, $h=1$ (without loss of generality), $p=5$ and $S=500$. Write a simulator to compute the average cost for setting $T=100$. Then, change $T$ to find a better value.

How large will the cost to server be? Motivate first that this does not depend on $T$, unless $T$ is in the order of magnitude as the simulation duration. Then make an estimate.
  
\begin{solution}
  
\begin{pyverbatim}
N = 10000
labda = 0.3
mu = 1
q0 = 0
T = 100

h = 1
p = 5
S = 500


def compute_cost(a, q0=0):
    d = np.zeros_like(a)
    Q = np.zeros_like(a)
    Q[0] = q0
    present = False  # extra employee is not in.
    queueing_cost = 0
    server_cost = 0
    setup_cost = 0
    for i in range(1, len(a)):
        if present:
            server_cost += p
            s = poisson(mu).rvs()
        else:
            s = 0
        d[i] = min(Q[i - 1], s)
        Q[i] = Q[i - 1] + a[i] - d[i]
        if Q[i] == 0:
            present = False  # send employee home
        elif Q[i] >= T:
            if present == False:  # server is switched on:
                setup_cost += S
            present = True
        queueing_cost += h * Q[i]

    print(queueing_cost, setup_cost, server_cost)

    total_cost = queueing_cost + server_cost + setup_cost
    num_periods = len(a) - 1
    average_cost = total_cost / num_periods
    return average_cost

a = poisson(labda).rvs(N)
av = compute_cost(a, q0)
print(av)
\end{pyverbatim}
After a bit of experimentation, we see that $T=15$ is quite a bit better.

The total cost of the server must be $ p \rho N = p \lambda/\mu N$. To see this, note that $\rho$ must be the fraction of the time the server is busy, hence $N\rho$ is the total time it is busy. Thus the cost must be about \py{5*0.3/1*10000} 
\end{solution}


\end{exercise}

\begin{itemize}
\item Time varying demand.
\end{itemize}


\begin{exercise}
  What have you learned from this tutorial?
  \begin{solution}
    \begin{enumerate}
    \item  Making a  simulation requires some ingenuity, but is, often, not difficult
    \item With simulation it becomes possible to analyze queueing situations. The mathematical analysis of often much harder. 
    \item We studied the behavior of queues under certain control policies, typically policies that change the service rate as a function of the queue length.
    \end{enumerate}
  \end{solution}
\end{exercise}

\clearpage

\section{A queueing network}
\label{sec:queueing-network}


\section{A queueing network with workload control}
\label{sec:queu-netw-with}




\section{Nurse routing}
\label{sec:nurse-routing}

shift sheduling. setups. 


\section{An Assembly line}
\label{sec:an-assembly-line}




\Closesolutionfile{ans}


\clearpage
\section*{Solutions}
\input{ans}


\end{document}

simulatie van single server queue

simulatie van lopende band, idee met Marien.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
