
\documentclass{scrartcl}
%\documentclass[a5paper]{article}

\usepackage[solutionfiles]{optional}
%\usepackage[nosolutionfiles]{optional}
\opt{nosolutionfiles}{\usepackage[nosolutionfiles]{answers}}
\opt{solutionfiles}{\usepackage{answers}}

\usepackage{preamble}

% handle indentation below figure captions
\setcapwidth[c]{.8\textwidth}
\setcapwidth{0.95\textwidth}
%\setcapindent{0pt}
%\addtokomafont{caption}{\centering}

%\usepackage[top=5mm, left=5mm, right=5mm, bottom=2cm]{geometry}

\newenvironment{pyconcodeblock}%
 {\VerbatimEnvironment
  \begin{VerbatimOut}{temp.py}}%
 {\end{VerbatimOut}%
  \pyconc{exec(compile(open('temp.py', 'rb').read(), 'temp.py', 'exec'))}%
  \inputpygments{python}{temp.py}}





\title{Queueing simulations}
\author{N.D. Van Foreest and R. Van Beesten}
\begin{document}
\maketitle

\begin{itemize}
\item document in de levelcrossing illustration directory. 
\item Kijk even goed wat ik eigenlijk allemaal al heb.
\item Overshoot verdeling? 
\end{itemize}

\section{Introduction}

Compution, modeling is very interesting, and intellectually challenging. 


Typically a queueing system is subject to rules about when to allow jobs to enter the system or to adapt the service capacity. Such a decision rule is typically called a \emph{policy}.  The theoretical analysis of the efficacy of policies is often very hard, while with simulation it becomes doable.  In this document we present a number of cases to see how simulation can be used to analyze and improve queueing systems. Besides that these the cases will improve your understanding of queueing systems and probability theory, they will also make clear  that simulation is a really creative activity and involves solving many interesting and challenging  algorithmic problems.  


Each case is organized in a number of exercises. For each exercise,
\begin{enumerate}
\item First make a design of how you want to solve the problem. For instance, make a model of  queueing system, or a control policy structure, or compute relevant KPIs (key performance indicators, such as cost, or utilization of the server, and so on). In other words, think before you type. 
\item Try to implement your ideas in python\footnote{Some of you might wonder why we use python rather than R. There are a few reasons for this. Python is more or less the third most used programming language, after c++ and java. It is widely used by companies, while R is hardly  used outside of academia. Programming OR applications is easier in python; it will also used in other courses. Finally, if you are interested in machine learning and artificial intelligence python is, hands-down, the best choice.}
  \item If you don't succeed in getting your program to work,  look up the code written by us and type it to your python environment.\footnote{Typing yourself forces you to read the code well.}.
  \item Simulate a number of scenarios
  \item Vary the parameter settings and see what happens, i.e., play with the numbers and see whether you can still understand the results. 
\end{enumerate}


We expect you to work in a groups of 2 to 3 students and bring a laptop with an \emph{installed and working} python environment, preferably  the anaconda package available at \url{https://www.anaconda.com/}, as this contains all functionality we will need\footnote{There are also python environments available on the web, such as repl.it., but that is typically a bit less practical than running the code on your own machine.}. You can find a nice tutorial to python programming at  \url{https://www.programiz.com/python-programming}. Note, this site advises to install python just by itself. We advise you instead to download anaconda instead, as this contains also the required numerical libraries. 

We will use the following libraries of python a lot:
\begin{itemize}
\item \pyv{numpy}  provides an enormous amount of functions to handle large (multi-dimensional) arrays with numbers. 
\item \pyv{scipy} contains numerical recipes, such as solvers for optimization software, solvers for differential equations. \pyv{scipy.stats} contains many probability distributions and numerical methods to operate on these functions. 
\item \pyv{matplotlib}  provides plotting functionality.
\end{itemize}

Our code is not the most efficient, or fast. Rather, we focus on clarity of code so that the underlying reasoning is as clear as possible. Once our ideas and code are correct, we can start optimizing, if this is necessary. 

Finally, the code is part of the course, hence of the midterms and the exams.  Unless indicated as not obligatory, you have to be able to read the code and understand it.

\Opensolutionfile{hint}
\Opensolutionfile{ans}

\section{Exponential distribution}

The aim of this tutorial is to show, empirically, a fascination fact: even for very small populations in which individuals decide independently to visit a server (a shop, a hospital, etc),  the  exponential distribution is a good model for the interarrival times as seen by the server.  We will develop a simulation to motivate this `fact of nature'.  In particular, our aim is to build Figures 1.1, 1.2., and 1.3 of the queueing book.  (Read the description that underlies these figures.)

One important step in this process is to compute the empirical distribution. As this is much more interesting (and challenging) than you might think\footnote{If you search the web, you will see that computing the empirical density function is even more challenging.}, we start with this. Once we can compute empirical distribution functions, we are in good shape to set up the rest of the simulation. 

\begin{exercise}
  Make a plan of the steps you have to carry out to make Figure 1.1. In the next set of exercises we'll carry out these steps. So please do not read on before having thought about this problem.

  Spend some 5 minutes to think about how to approach the problem and how to chop it up into simple steps. Then organize the steps into a logical sequence. Don't worry at first about how to convert your ideas into computer code. Coding is a separate activity.  (As a matter of fact, I always start with making a small plan on how to turn an idea into code, and I call it `modeling'. This is not simple, typically.) 

  \begin{solution}
    \begin{enumerate}
    \item Generate  uniform distributed random variables for one customer corresponding to interarrival times.
    \item Plot the interarrival times
    \item Compute (empirical) distribtuion function of the random interarrival times.
    \item Plot the (empirical) distribtuion function
    \item Generate  uniform distributed random variables for multiple customers, e.g., 3. 
    \item Compute the arrival times for each customer.
    \item Merge  the arrival times for all customers. This is the arrival process as seen by the shop.
    \item Compute the interarrival times as seen by the shop
    \item Plot these interarrival times.
    \item Compare to the exponential density with a suitable arrival rate $\lambda$. 
    \end{enumerate}
  \end{solution}
\end{exercise}

\subsection{Empirical distributions}
\label{sec:empir-distr}

Before designing an algorithm, it is best to start with a simple numerical example and try to formalize the steps we take in the process.

\begin{exercise}
  Suppose you are given the following numbers: \pyb{a = [3.2, 4, 4, 1.3, 8.5, 9]}, what steps  do you  take to make the empirical distribution function? Recall, this is defined as
  \begin{equation}
    \label{eq:1}
    F(x) = \frac{\# \{i : a_i \leq x\}}{n}, 
  \end{equation}
  with $n$ is the length of $a$.

Can you turn it into an algorithm? (Just attempt to design an algorithm. Even if you don't succeed, trying is important.)

  \begin{solution}
We put the algorithm in a function so that we can use it later.
    \begin{pyblock}
def cdf(a):
    a = sorted(a)
    m, M = int(min(a)), int(max(a))+1
    # Since we know that a is sorted, this next line 
    # would be better, but less clear perhaps: 
    # m, M = int(a[0]), int(a[-1])+1 

    F = dict() # store the function i \to F[i]
    F[m-1]=0  # since F[x] = 0 for all x < m
    i = 0
    for x in range(m, M):
        F[x] = F[x-1]
        while i< len(a) and a[i] <= x:
            F[x] += 1
            i += 1

    # normalize
    for x in F.keys(): 
        F[x] /= len(a)

    return F
    \end{pyblock}

If we run this
\begin{pyblock}
F = cdf(a)
print(F)
\end{pyblock}
we get
\printpythontex
  \end{solution}
\end{exercise}

\begin{exercise}
  The method provided by the (solution of the) previous exercise is simple, but not completely correct. What is wrong?
  \begin{solution}
    We have to guess the support of $F$ (the set of points where $F$ makes the jumps) upfront, and we concentrated the support on the integers. However $F$ makes jumps at floats, for instance  at $3.2$. 
  \end{solution}
\end{exercise}

A better idea is to consider the sorted version of $a$. 

\begin{exercise}
  Sort the numbers in the list $a$, and make a plot (by hand) of $a$ as a function of the index, that is, plot the function $i\to \text{sorted}(a_i)$, with $i$ on the $x$-axis, and $a_i$ on the $y$-axis.  Observe this is the inverse of $F$ (except for the normalization).
  \begin{solution}
    In the answer we let the computer do all the work.  

\begin{pyblock}
# we first need to fix some initial settings.
import numpy as np
import matplotlib
matplotlib.use('pdf') 
import matplotlib.pyplot as plt

I = range(0, len(a))
plt.plot(I, sorted(a))
plt.savefig("dummy_plot_1.pdf")
plt.cla() # clear the figure so that we can overwrite it later.
\end{pyblock}

\begin{center}
\includegraphics[scale=0.5]{dummy_plot_1.pdf}
\end{center}
  \end{solution}
\end{exercise}

\begin{exercise}
  Find now a way to invert $i\to a_i$, normalize the function to get a distribution, and make a new plot. 
  \begin{solution}
Here is one way. Note that we already imported matplotlib, so we don't have to that again.
\begin{pyblock}
def cdf(a):  
    y = range(1, len(a)+1)
    y = [yy/len(a) for yy in y] # normalize
    x = sorted(a)
    return x, y

x, y = cdf(a)

plt.plot(x, y)
plt.savefig("dummy_plot_2.pdf")
plt.cla() # clear the figure so that we can overwrite it later.
\end{pyblock}
Why do we start $y$ with 1?


\begin{center}
\includegraphics[scale=0.5]{dummy_plot_2.pdf}
\end{center}
  \end{solution}
\end{exercise}

We now compute and plot a distribution function of interarrival times specified by a list (vector, array) $a$. For our present goals this suffices. If, however, you like details, you should notice that our plot of the distribution function is still not entirely OK:  the graph should make  jumps, but it doesn't.  In the rest of this section we show how to repair this, and we discuss a faster way to compute the cdf. You can skip this if you are not interested. 

\begin{exercise}
Read about the \pyv{drawstyle} option of  the plot function of matplotlib to see how to make jumps.
  \begin{solution}
With the drawstyle option: 
\begin{pyblock}
plt.plot(x, y,  drawstyle = 'steps-pre')
plt.savefig("dummy_plot_drawstyle.pdf")
plt.cla() # clear the figure so that we can overwrite it later.
\end{pyblock}


\begin{center}
\includegraphics[scale=0.5]{dummy_plot_drawstyle.pdf}
\end{center}

But now we still have  vertical lines. To remove those, we can use \pyv{hlines}.

\begin{pyblock}
y = range(0, len(a)+1)
y = [yy/len(a) for yy in y] # normalize
a = sorted(a)
left = [0] + a
right = a + [10]

plt.hlines(y, left, right)
plt.savefig("dummy_plot_lines.pdf")
plt.cla() # clear the figure so that we can overwrite it later.
\end{pyblock}

\begin{center}
\includegraphics[scale=0.5]{dummy_plot_lines.pdf}
\end{center}

There  we are!.
  \end{solution}
\end{exercise}


\begin{exercise}
Finally, we can make the computation of the cdf significantly faster with using the following numpy functions. 
\begin{enumerate}
\item \pyv{numpy.unique}
\item \pyv{numpy.sort}
\item \pyv{numpy.cumsum}
\item \pyv{numpy.sum}
\end{enumerate}
How can you use these to compute the cdf?
\begin{solution}
Here it is.
\begin{pyblock}

def cdf(X):
    # remove multiple occurences of the same value
    unique, count = np.unique(np.sort(X), return_counts=True)
    x = unique
    y = count.cumsum()/count.sum()
    return x, y

x, y = cdf(a)
\end{pyblock}

\end{solution}
\end{exercise}

\subsection{Simulating the arrival process of a single customer}
\label{sec:simulations}

The next step is to simulate interarrival times of a single customer and  make an empirical cdf of these times.  Then we graphically compare this cdf to the exponential distribution. A more formal method to compare cdfs is by means of the Kolmogorov-Smirnov statistic (see wikipedia) which we develop in passing.

\begin{exercise}
  Generate 3 random numbers uniformly distributed on $[4,6]$.  Print these numbers to see if you get something decent. Read the documentation, on the web, of:
 \pyv{scipy.stats.uniform}.  Check in particular the \pyv{rvs()} function. 

\begin{solution}
Copy this code and run it.
\begin{pyverbatim}
# we first need to fix some initial settings.
import numpy as np
import scipy
import matplotlib
matplotlib.use('pdf') 
import matplotlib.pyplot as plt
from scipy.stats import uniform

np.set_printoptions(precision=3) # this is to print not too many digits to the screen

# fix the seed
scipy.random.seed(3) 

# parameters
L = 3  # number of interarrival times

G = uniform(loc=4, scale=2) # G is called a frozen distribution.
a = G.rvs(L)
print(a)
\end{pyverbatim}
  
\end{solution}

\end{exercise}


\begin{exercise}
Generate $L=300$ random numbers $\sim U[4,6]$ and plot these. You should interpret these random numbers as interarrival times of one customer at a shop (in hours say.)
\begin{solution}
Add this code to the other code and run it.
\begin{pyverbatim}
N = 1 # number of customers
L = 300
a = G.rvs(L)

plt.plot(a, label="a")
plt.title("N = {}, L = {}".format(N, L))
plt.legend()
plt.savefig("dummy_plot.pdf")
plt.cla() # clear the figure so that we can overwrite it later.
\end{pyverbatim}
I use the same name for the pdf file, so to prevent having to delete 10 figures or so. If you don't like this, just provide another name. 
\end{solution}
\end{exercise}

\begin{exercise}
Compute  the empirical distribution function of these interarrival times, and plot the cdf.
\begin{solution}
Add this to the other code and run it.
\begin{pyverbatim}
x, y = cdf(a)
plt.plot(x,y,  label="d")
plt.legend()
plt.savefig("dummy_plot.pdf")
plt.cla() # clear the figure
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  We like to numerically compare the empirical distribution of the interarrival tiems to the theoretical distribution, which is uniform in this case. 
For this we can use the Kolmogorov-Smirnov statistic. Try to come up with a method to compute this statistic, and then compute it. 

You might find the following functions helpful (read the documentation on the web to see what they do):
\begin{enumerate}
\item \pyv{numpy.max}
\item \pyv{numpy.abs}
\item \pyv{scipy.stats.uniform}, the \pyv{cdf} function.
\end{enumerate}

\begin{solution}
Add this to the other code and run it.
\begin{pyverbatim}
def KS(X, F):
    # Compute the Kolmogorov-Smirnov statistic where
    # X are the data points
    # F is the theoretical distribution
    support, y = cdf(X)
    y_theo = np.array([F.cdf(x) for x in support])
    return np.max(np.abs(y-y_theo))

print(KS(a, G))    
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  Now compute the KS statistics to compare the simulated interarrival times with an exponential distribution with a suitable mean. (What is this suitable mean?).

See \pyv{scipy.stats.expon}.


\begin{solution}
Add this to the other code and run it. Since the mean interarrival time is $5$, take $\lambda = 1/5$.

\begin{pyverbatim}
from scipy.stats import expon

labda = 1./5 # lambda is a function in python
E = expon(scale=1/labda) 
print(E.mean()) # to check that we chose the right scale
print(KS(a, E))    
\end{pyverbatim}
\end{solution}
\end{exercise}

\begin{exercise}
  Finally, plot the empirical distribution and the exponential distribution in one graph. Explain why these graphs are different.
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
x, y = cdf(a)
plt.plot(x, y,  label="empirical")
plt.plot(x, E.cdf(x),  label="exp")
plt.legend()
plt.savefig("dummy_plot.pdf")
plt.cla() # clear the figure
\end{pyverbatim}

It is pretty obvious why these graphs must be different: we compare a uniform and an exponential distribution. 
\end{solution}
\end{exercise}


\subsection{Simulating many customers}
\label{sec:simul-many-cust}

We now like to simulate the interarrival process as seen by a shop that serves many customers. For ease, we call this the merged, or superposed, interarrival process. Again, this requires quite a bit of thought. Thus, we start with a numerical example with two customers, and trace and organize all steps we make to compute the empirical distribution of the merged interarrival process. Then we make an algorithm, and scale up to many numbers. 

\begin{exercise}
  Suppose we have two customers with interarrival times $a=[4, 3, 1.2, 5]$ and $b=[2, 0.5, 9]$. Make, by hand, the empirical cdf of the merged process.
  \begin{hint}
  The shop sees the combined arrival process, so  first find a way to merge the arrival times of the customers into one arrival process as observed by the shop. Then  convert this into interrival times at the shop. 
  \end{hint}
  \begin{solution}
    The following steps in code explain the logic.
    \begin{pyblock}
a=[4, 3, 1.2, 5]
b=[2, 0.5, 9]

def compute_arrivaltimes(a):
    A=[0]
    i = 1
    for x in a:
        A.append(A[i-1] + x)
        i += 1

    return A

A = compute_arrivaltimes(a)
B = compute_arrivaltimes(b)


times = set()
times.update(A) # add the arrival times of A 
times.update(B)
times = sorted(times) # ensure that the times are in increasing order

shop = set() # a set of interarrival times at the shop
for s, t in zip(times[:-1], times[1:]):
    shop.add(t-s) # t-s is an interarrival time

print(shop)
    \end{pyblock}
  \end{solution}
\end{exercise}


\begin{exercise}
  The steps of the previous exercise can be summarized by this code:
\begin{pyverbatim}
def superposition(a):
    A = np.cumsum(a, axis=1)
    A = np.sort(np.union1d(A))
    return np.diff(A)

\end{pyverbatim}  

Try to understand this by reading the documentation (on the web) of the following functions.
\begin{enumerate}
\item \pyv{numpy.cumsum}, in particular read about the meaning of axis.
\item \pyv{numpy.union1d}
\item \pyv{numpy.diff}
\end{enumerate}
\end{exercise}


\begin{exercise}
  Generate 100 random interarrival times for 3 customers, plot the cdf of the merged process, and compare to the exponential distribution with the correct mean. What is the effect of increasing the number of customers from $N=1$ to $N=3$?
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
N, L = 3, 100
a = superposition(G.rvs((N, L)))

E = expon(scale=1./(N*labda)) # the arrival rate of N customers is N times as large
print(E.mean())

x, y = cdf(a)
plt.title("N = {}, L = {}".format(N, L))
plt.plot(x, y,  label="empirical")
plt.plot(x, E.cdf(x),  label="exp")
plt.legend()
plt.savefig("dummy_plot.pdf")
plt.cla() # clear the figure
\end{pyverbatim}

\end{solution}
\end{exercise}


\begin{exercise}
  Compare  the empirical distribution of the interarrival times generated by  $N=10$ customers to the exponential distribution (compute the appropriate arrival rate). Make a plot, and explain what you see.
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
N, L = 10, 10
a = superposition(G.rvs((N, L)))

E = expon(scale=1/(N*labda))

x, y = cdf(a)
plt.title("N = {}, L = {}".format(N, L))
plt.plot(x, y,  label="empirical")
plt.plot(x, E.cdf(x),  label="exp")
plt.legend()
plt.savefig("dummy_plot.pdf")
plt.cla() # clear the figure
\end{pyverbatim}

This is great. For just $N=10$ we see that the exponential distribution is a real good fit. 
\end{solution}
\end{exercise}



\begin{exercise}
  Do the same for $N=10$ customers with normally distributed interarrival times with $\mu=5$, and $\sigma =1$.
For this use \pyv{scipy.stats.norm}. What do you see? What is the influence of the distribution of interarrival times of an individual customer? 
\begin{solution}
Add this to the other code and run it. 
\begin{pyverbatim}
from scipy.stats import norm

N, L = 10, 10
a = superposition(norm(loc=5, scale=1).rvs((N, L)))

plt.title("N = {}, L = {}".format(N, L))
plt.plot(x, y,  label="empirical")
plt.plot(x, E.cdf(x),  label="exp")
plt.legend()
plt.savefig("dummy_plot.pdf")
plt.cla() # clear the figure
\end{pyverbatim}

Clearly, whether the distribution of interarrival times of an individual customer are uniform, or normal, doesn't really matter. In both cases the exponential distribution is a good model for what the shop sees. We did not analyze what happens if we would merge customers with normal distribution and uniform distribution, but we can suspect that in all these cases the merged process converges to a set of i.i.d. exponentially distributed random variables.

\end{solution}
\end{exercise}

\begin{exercise}
If  $\mu=\sigma=5$ then the analysis should break down. What happens if you set $\sigma=5$? If you don't get real strange results, the code itself must be wrong. (As an aside, when testing code it is good to see what happens if you use bogus numbers. The program should fail.)
\end{exercise}

\begin{exercise}
  Make a summary of what you have learned from this tutorial.
  \begin{solution}
    \begin{enumerate}
    \item Algorithmic thinking, i.e., how to chop up a computational challenge into small steps.
    \item How to efficiently compute the empirical distribution function
    \item The Kolmogorov-Smirnov statistic
    \item The empirical distribution of a  merged interarrival arrival process converges, typically, super rapidly to an exponential distribution. Thus,  interarrival times at shops, hospitals and so on, are often very well described by an exponential distribution with suitable mean. 
    \item This convergence is not so sensitive to the distribution of the interarrival times of a single customers. For the shop, only the population matters. 
    \item Using functions (e.g., \pyv{def compute(a)})  to document code  (by the function name), hide complexity, and reuse code so that it can be applied multiple times. Moreover, defining functions is in line with the extremely important Dont-Repeat-Yourself (DRY) principle. 
    \item Python, numpy and scipy coding skills
    \end{enumerate}
  \end{solution}
\end{exercise}


%\subsection{Memoryless property}

% It is well known that the exponential distribution has the memoryless property. Does it hold for simulations too? 

% First generate our standard case.

% \begin{pyblock}
% N = 10
% runlength = 1000
% labda = N/m
% d = m
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% a = superposition(G, N, runlength)
% \end{pyblock} 

% Now select the interarrival times that exceed some number $x$ and
% subtract $x$. These new values should, hopefully, have the same
% distribution as the initial set of interarrival times.

% \begin{pyblock}
% x = 0.5 # threshold
% c = a[a>x] -x # select the interarrival times longer than x
% \end{pyblock} 

% Let's plot it
% \begin{pyblock}
% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% This is quite ok. What is the threshold $x$ becomes larger, e.g., 1?

% \begin{pyblock}
% x = 2. # threshold
% c = a[a>x] -x # select the interarrival times longer than x

% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% Now the result is not as nice. What if we increase the simulation length? 
% \begin{pyblock}
% N = 10
% runlength = 100000
% labda = N/m
% d = m
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% a = superposition(G, N, runlength)

% x = 2. # threshold
% c = a[a>x] -x # select the interarrival times longer than x

% e = empiricalDistribution(c)
% plt.plot(e.x, e.cdf , label = "emp")
% plt.plot(e.x,1.-np.exp(-labda*e.x), label = "th")
% plt.legend()
% plt.show()
% \end{pyblock} 

% It is interesting to see that now the memoryless property seems to
% hold again. But why do we need so many values to see this? 



% \subsection{Analysis of the number arrivals in an interval}

% Now I build the same data, but count the number of arrivals that occur in a certain interval. 

% First I compute, as above, a number of arrival times as seen by the counter.

% \begin{pyblock}
% m = 5.
% d = 2.
% G = stats.uniform(loc = m-d, scale = 2.*d) 

% N = 300
% runlength = 1000
% a = G.rvs((N, runlength))
% A = np.cumsum(a, axis = 1)
% A = np.sort(A.flatten())
% \end{pyblock} 

% Next, I make a histogram, that, is I chop up the entire simulation
% interval, from the first arrival time to the last, into a number of
% bins of equal length. Then I use $np.histogram$ to count the number of
% arrivals in each such interval.  There are $N*runlength$ arrivals in
% total.  I like the bins of such size that on average each bin will
% contain 50 arrivals. Thus, the number of bins, i.e., the number of intervals in which the entire simulation interval needs to chopped, should be $N*runlength/50$. 

% \begin{pyblock}
% #bins = N*runlength/50
% #p, x = np.histogram(A, bins = bins)
% \end{pyblock} 

% Here, $x$ contains the interval boundaries in which the simulation
% interval is chopped up. Therefore $I=x[1]-x[0]$ is the length of one
% such interval. The arrival rate in such interval must be $I*N/m$, as
% $N/m$ is the arrival rate per unit time. Therefore,
% \begin{pyblock}
% #I = x[1]- x[0]
% #labda = float(I)*N/m
% \end{pyblock} 

% Now I count the number of times a certain number of arrivals occured.
% \begin{pyblock}
% P =  np.bincount(p)
% \end{pyblock} 

% Finally, I want to fit a Poisson distribution to see how well the
% Poisson distribution fits the data.  I need the support of the
% measured data, and store in $x$. As $np.bincount$ only counts, it is
% necessary to normalize $P$.
% \begin{pyblock}
% x = range(p.min(), p.max())

% plt.plot(P/float(sum(P)))
% plt.plot(x, scipy.stats.poisson.pmf(x, labda)) 
% plt.show()
% \end{pyblock} 

% Not bad. However, some experimentation shows that when $N$ is small,
% like 30 or so, and the $runlength$ is short, in the order of 100 or
% so, the quality of the Poisson approximation is much less. I do not
% fully understand why that is the case.

% \begin{pyblock}
% N = 30
% runlength = 300
% a = G.rvs((N, runlength))
% A = np.cumsum(a, axis = 1)
% A = np.sort(A.flatten())

% bins = N*runlength/50
% p, x = np.histogram(A, bins = bins)

% I = x[1]- x[0]
% labda = float(I)*N/m

% P =  np.bincount(p)
% plt.plot(P/float(sum(P)))

% x = range(p.min(), p.max())
% plt.plot(x, scipy.stats.poisson.pmf(x, labda)) 
% plt.show()
% \end{pyblock} 




\section{A single server queue}
\label{sec:single-server-queue}

\begin{itemize}
\item Single server queue
\item Maak de empirische verdelingsfunctie van de queue lengte voor de M/D/1 queue
\item Control zoals bij de psychiaters. Maak de empirische verdelingsfunctie van de wachttijd voor deze queue.
\item Simulation of a D policy of wachttijd berekening ipv queue lengte.
\end{itemize}


\section{Heat bath}
\label{sec:heat-bath}


\section{A queueing network}
\label{sec:queueing-network}


\section{A queueing network with workload control}
\label{sec:queu-netw-with}


\section{Nurse routing}
\label{sec:nurse-routing}

shift sheduling. setups. 


\section{An Assembly line}
\label{sec:an-assembly-line}




\Closesolutionfile{hint}
\Closesolutionfile{ans}

\clearpage
\section*{Hints}
\input{hint}

\clearpage
\section*{Solutions}
\input{ans}


\end{document}

simulatie van single server queue

simulatie van lopende band, idee met Marien.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
